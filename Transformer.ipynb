{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Review!!\n",
    "\n",
    "1. Self-Attention Layer Advantage: parallel computation\n",
    "2. query, key, value interactions\n",
    "3. Positional Encoding\n",
    "4. Decoder has some properties just like RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [References](https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## package used\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint ## print beautifully\n",
    " \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install tensorflow-gpu == 2.0.0-beta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make tensorflow not that annoying\n",
    "import logging\n",
    "logging.basicConfig(level = 40)\n",
    "\n",
    "## make numpy show numbers that are more intuitive\n",
    "np.set_printoptions(suppress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"Trans\"\n",
    "if output_dir not in os.listdir():\n",
    "    os.makedirs(output_dir) ## make the dir\n",
    "\n",
    "## define path that we are going to store\n",
    "\n",
    "en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "download_dir = \"tensorflow-datasets/downloads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Split('train'): ['newscommentary_v14',\n",
      "                  'wikititles_v1',\n",
      "                  'uncorpus_v1',\n",
      "                  'casia2015',\n",
      "                  'casict2011',\n",
      "                  'casict2015',\n",
      "                  'datum2015',\n",
      "                  'datum2017',\n",
      "                  'neu2017'],\n",
      " Split('validation'): ['newstest2018']}\n"
     ]
    }
   ],
   "source": [
    "tmp_builder = tfds.builder(\"wmt19_translate/zh-en\")\n",
    "pprint(tmp_builder.subsets) ## dataset avaliable for zh_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TFDS_API](https://www.tensorflow.org/datasets/catalog/overview#translate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Using custom data configuration zh-en\n"
     ]
    }
   ],
   "source": [
    "config = tfds.translate.wmt.WmtConfig(\n",
    "            version=tfds.core.Version(\"1.0.0\"),\n",
    "            language_pair=(\"zh\", \"en\"),\n",
    "            subsets={\n",
    "                tfds.Split.TRAIN: [\"newscommentary_v14\"]\n",
    "            }\n",
    "        )\n",
    "builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "builder.download_and_prepare(download_dir=download_dir)\n",
    "\n",
    "## ther so miuch warning are actually helping us get rid of abnormal format of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data are too big, just use a few of them\n",
    "split = ['train[:20%]', 'train[20%:21%]']\n",
    "train_examples, val_examples = builder.as_dataset(split=split, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'The fear is real and visceral, and politicians ignore it at their peril.', shape=(), dtype=string)\n",
      "\n",
      "tf.Tensor(b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## take a peak of it\n",
    "for en, zh in train_examples.take(1):\n",
    "    print(en)\n",
    "    print()\n",
    "    print(zh)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fear is real and visceral, and politicians ignore it at their peril.\n",
      "这种恐惧是真实而内在的。 忽视它的政治家们前途堪忧。\n",
      "\n",
      "In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word “liberal” – a champion of the cause of individual freedom.\n",
      "事实上，德国政治局势需要的不过是一个符合美国所谓“自由”定义的真正的自由党派，也就是个人自由事业的倡导者。\n",
      "\n",
      "Shifting to renewable-energy sources will require enormous effort and major infrastructure investment.\n",
      "必须付出巨大的努力和基础设施投资才能完成向可再生能源的过渡。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## what the f*ck is that? That us decode it out\n",
    "\n",
    "sample = []\n",
    "num_sample = 1\n",
    "\n",
    "for en, zh in train_examples.take(3):\n",
    "    en = en.numpy().decode(\"utf-8\")\n",
    "    zh = zh.numpy().decode(\"utf-8\")\n",
    "    \n",
    "    print(en)\n",
    "    print(zh)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create bag of words index dict\n",
    "\n",
    "## use the wordpieces model (GNMT)\n",
    "try:\n",
    "    ## next time we will just read the \"bags\"\n",
    "    subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
    "except:\n",
    "\n",
    "    subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "          (en.numpy() for en, _ in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "    # save ur result for next used\n",
    "    subword_encoder_en.save_to_file(en_vocab_file)\n",
    "\n",
    "    print(f\"size of bags：{subword_encoder_en.vocab_size}\")\n",
    "    print(f\"examples：{subword_encoder_en.subwords[:10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125, 1558, 8, 1166, 7960]\n"
     ]
    }
   ],
   "source": [
    "## try it?\n",
    "sample_sentence = \"I am a pig\"\n",
    "indices = subword_encoder_en.encode(sample_sentence)\n",
    "\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I ', 'am ', 'a ', 'pi', 'g']\n"
     ]
    }
   ],
   "source": [
    "## back?\n",
    "\n",
    "sample_sentence = []\n",
    "for idx in indices:\n",
    "    subwords = subword_encoder_en.decode([idx])\n",
    "    sample_sentence.append(subwords)\n",
    "    \n",
    "print(sample_sentence)\n",
    "\n",
    "## words that never appear in the corpus will be deconstructed\n",
    "## this kind of word to poeces method are more flexible while facing unseen words\n",
    "\n",
    "## decode and encode just works like an 1-1 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## time for zh\n",
    "\n",
    "try:\n",
    "    ## next time we will just read the \"bags\"\n",
    "    subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n",
    "except:\n",
    "\n",
    "\n",
    "    subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "          (zh.numpy() for _, zh in train_examples), target_vocab_size=2**13,\n",
    "            max_subword_length = 1 ## zh doesnot have more than one subword\n",
    "    )\n",
    "\n",
    "    # save ur result for next used\n",
    "    subword_encoder_zh.save_to_file(zh_vocab_file)\n",
    "\n",
    "    print(f\"size of bags：{subword_encoder_zh.vocab_size}\")\n",
    "    print(f\"examples：{subword_encoder_zh.subwords[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 6, 7, 223, 2078]\n",
      "['我', '是', '一', '只', '猪']\n"
     ]
    }
   ],
   "source": [
    "## 這句話沒辦法翻譯我打中文\n",
    "## 可悲沒有繁體字\n",
    "\n",
    "## similarly, we sample to check 1-1 propeties\n",
    "\n",
    "sample_sentence = \"我是一只猪\"\n",
    "indices = subword_encoder_zh.encode(sample_sentence)\n",
    "print(indices)\n",
    "\n",
    "sample_sentence = []\n",
    "for idx in indices:\n",
    "    subwords = subword_encoder_zh.decode([idx])\n",
    "    sample_sentence.append(subwords)\n",
    "    \n",
    "print(sample_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "I am a pig\n",
      "我是一只猪\n",
      "\n",
      "After:\n",
      "[125, 1558, 8, 1166, 7960]\n",
      "[65, 6, 7, 223, 2078]\n"
     ]
    }
   ],
   "source": [
    "## concate above, we now have\n",
    "sample_sent_en = \"I am a pig\"\n",
    "sample_sent_zh = \"我是一只猪\"\n",
    "\n",
    "indice_en = subword_encoder_en.encode(sample_sent_en)\n",
    "indice_zh = subword_encoder_zh.encode(sample_sent_zh)\n",
    "\n",
    "print(\"Before:\")\n",
    "print(sample_sent_en)\n",
    "print(sample_sent_zh)\n",
    "print()\n",
    "print(\"After:\")\n",
    "print(indice_en)\n",
    "print(indice_zh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding token representing the start and end, <BOS> and <EOS>\n",
    "\n",
    "## input -> two string temsor\n",
    "## output -> two indices with BOS,EOS\n",
    "def encode(en_t, zh_t):\n",
    "    en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n",
    "    en_t.numpy()) + [subword_encoder_en.vocab_size + 1]\n",
    "    \n",
    "    zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(\n",
    "    zh_t.numpy()) + [subword_encoder_zh.vocab_size + 1]\n",
    "    \n",
    "    return en_indices, zh_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en BOS index： 8113\n",
      "en EOS index： 8114\n",
      "zh BOS index： 4205\n",
      "zh EOS index： 4206\n",
      "\n",
      "b'The fear is real and visceral, and politicians ignore it at their peril.'\n",
      "b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82'\n",
      "\n",
      "[8113, 16, 1284, 9, 243, 5, 1275, 1756, 156, 1, 5, 1016, 5566, 21, 38, 33, 2982, 7965, 7903, 8114]\n",
      "[4205, 10, 151, 574, 1298, 6, 374, 55, 29, 193, 5, 1, 3, 3981, 931, 431, 125, 1, 17, 124, 33, 20, 97, 1089, 1247, 861, 3, 4206]\n"
     ]
    }
   ],
   "source": [
    "en_t, zh_t = next(iter(train_examples))\n",
    "en_indices, zh_indices = encode(en_t, zh_t)\n",
    "\n",
    "print('en BOS index：', subword_encoder_en.vocab_size)\n",
    "print('en EOS index：', subword_encoder_en.vocab_size + 1)\n",
    "print('zh BOS index：', subword_encoder_zh.vocab_size)\n",
    "print('zh EOS index：', subword_encoder_zh.vocab_size + 1)\n",
    "print()\n",
    "print(en_t.numpy())\n",
    "print(zh_t.numpy())\n",
    "print()\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "## but, we cannot use it directly to our dataset because tf functions are graph mode\n",
    "## does not have numpy\n",
    "\n",
    "## def new one for tf\n",
    "\n",
    "def tf_encode(en_t, zh_t):\n",
    "    return tf.py_function(encode, [en_t,zh_t], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[8113   16 1284    9  243    5 1275 1756  156    1    5 1016 5566   21\n",
      "   38   33 2982 7965 7903 8114], shape=(20,), dtype=int64)\n",
      "\n",
      "tf.Tensor(\n",
      "[4205   10  151  574 1298    6  374   55   29  193    5    1    3 3981\n",
      "  931  431  125    1   17  124   33   20   97 1089 1247  861    3 4206], shape=(28,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "## what we done?\n",
    "tmp_dataset = train_examples.map(tf_encode)\n",
    "en_indices, zh_indices = next(iter(tmp_dataset))\n",
    "\n",
    "print(en_indices)\n",
    "print()\n",
    "print(zh_indices)\n",
    "\n",
    "## tmp outputs two indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in order to speed up ! remove all sentence has 40 ups indices\n",
    "\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(en, zh, max_length = MAX_LENGTH):\n",
    "    ## return TRUE, FALSE\n",
    "    return tf.logical_and(tf.size(en) <= max_length,tf.size(zh) <= max_length)\n",
    "\n",
    "## return TRUE values\n",
    "tmp_dataset = tmp_dataset.filter(filter_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 29784 data below 40 tokens\n"
     ]
    }
   ],
   "source": [
    "num_examples = 0\n",
    "\n",
    "## use for, but not recommanded\n",
    "for en_indices, zh_indices in tmp_dataset:\n",
    "    cond1  = len(en_indices) <= MAX_LENGTH\n",
    "    cond2 = len(zh_indices) <= MAX_LENGTH\n",
    "    \n",
    "    assert cond1 and cond2\n",
    "    num_examples += 1\n",
    "\n",
    "print(f\"we have {num_examples} data below {MAX_LENGTH} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en indices batch\n",
      "tf.Tensor(\n",
      "[[8113   16 1284 ...    0    0    0]\n",
      " [8113 1894 1302 ...    0    0    0]\n",
      " [8113   44   40 ...    0    0    0]\n",
      " ...\n",
      " [8113  122  506 ...    0    0    0]\n",
      " [8113   16  215 ...    0    0    0]\n",
      " [8113 7443 7889 ...    0    0    0]], shape=(64, 39), dtype=int64)\n",
      "\n",
      "zh indicesbatch\n",
      "tf.Tensor(\n",
      "[[4205   10  151 ...    0    0    0]\n",
      " [4205  206  275 ...    0    0    0]\n",
      " [4205    5   10 ...    0    0    0]\n",
      " ...\n",
      " [4205   34    6 ...    0    0    0]\n",
      " [4205  317  256 ...    0    0    0]\n",
      " [4205  167  326 ...    0    0    0]], shape=(64, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "## cuz our data has different length, building batch may occur some errors\n",
    "## padding as we do in image\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "## padding to be the same length\n",
    "## padding as same length to the longest length\n",
    "tmp_dataset = tmp_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "en_batch, zh_batch = next(iter(tmp_dataset))\n",
    "print(\"en indices batch\")\n",
    "print(en_batch)\n",
    "print()\n",
    "print(\"zh indicesbatch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine above all to create out train and valid\n",
    "MAX_LENGTH = 40\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 15000\n",
    "\n",
    "train_dataset = (train_examples  \n",
    "                 .map(tf_encode) \n",
    "                 .filter(filter_max_length)\n",
    "                 .cache() # speed up reading \n",
    "                 .shuffle(BUFFER_SIZE) # break origin order\n",
    "                 .padded_batch(BATCH_SIZE, \n",
    "                               padded_shapes=([-1], [-1]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)) # speed up\n",
    "\n",
    "val_dataset = (val_examples\n",
    "               .map(tf_encode)\n",
    "               .filter(filter_max_length)\n",
    "               .padded_batch(BATCH_SIZE, \n",
    "                             padded_shapes=([-1], [-1])))\n",
    "\n",
    "## cache and AUTOTUNE\n",
    "## https://www.tensorflow.org/guide/data_performance?hl=zh_cn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en indices batch\n",
      "tf.Tensor(\n",
      "[[8113   16 1547 ...    0    0    0]\n",
      " [8113   87    9 ...    0    0    0]\n",
      " [8113 1636   54 ...    0    0    0]\n",
      " ...\n",
      " [8113   87 5185 ...    0    0    0]\n",
      " [8113 7646 2120 ...    0    0    0]\n",
      " [8113  624    7 ...    0    0    0]], shape=(128, 39), dtype=int64)\n",
      "\n",
      "zh indices batch\n",
      "tf.Tensor(\n",
      "[[4205  201  160 ...    0    0    0]\n",
      " [4205   30    4 ...    0    0    0]\n",
      " [4205  286   12 ...    0    0    0]\n",
      " ...\n",
      " [4205   10  269 ...    0    0    0]\n",
      " [4205   34    6 ...    0    0    0]\n",
      " [4205    7   66 ...    0    0    0]], shape=(128, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "## take a peak and inagime what's going on!\n",
    "\n",
    "en_batch, zh_batch = next(iter(train_dataset)) ##return in one batch\n",
    "print(\"en indices batch\")\n",
    "print(en_batch)\n",
    "print()\n",
    "print(\"zh indices batch\")\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I am a pig', '我是一只猪'), ('The numbers speak for themselves.', '数字证明了一切。')]\n"
     ]
    }
   ],
   "source": [
    "## To help us into tensorflow world\n",
    "## make some examples to track!\n",
    "\n",
    "demo_examples = [\n",
    "    (\"I am a pig\",\"我是一只猪\"),\n",
    "    (\"The numbers speak for themselves.\", \"数字证明了一切。\")    \n",
    "]\n",
    "\n",
    "print(demo_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "## make it to tensor\n",
    "demo_examples = tf.data.Dataset.from_tensor_slices((\n",
    "    [en for en, _ in demo_examples], [zh for _, zh in demo_examples]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  125 1558    8 1166 7960 8114    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "\n",
      "tar: tf.Tensor(\n",
      "[[4205   65    6    7  223 2078 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "## same as above procedure\n",
    "demo_dataset = demo_examples.map(tf_encode).padded_batch(batch_size, padded_shapes=([-1], [-1]))\n",
    "inp, tar = next(iter(demo_dataset))\n",
    "print('inp:', inp)\n",
    "print()\n",
    "print('tar:', tar)\n",
    "\n",
    "## shape = (batch_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       " array([[[-0.03639312, -0.02757695,  0.04359658, -0.03537921],\n",
       "         [ 0.01971773, -0.0400288 , -0.03068806,  0.00325962],\n",
       "         [ 0.02722507, -0.02708452,  0.03049209, -0.02288125],\n",
       "         [-0.03032577, -0.0354887 ,  0.03697484,  0.02999594],\n",
       "         [ 0.01879238,  0.02930457,  0.04095887, -0.04010413],\n",
       "         [ 0.03148792, -0.02128877, -0.02220495,  0.01309374],\n",
       "         [-0.00649624, -0.02425881, -0.04920071,  0.04663258],\n",
       "         [ 0.04220078,  0.02385452, -0.04670385,  0.04206574]],\n",
       " \n",
       "        [[-0.03639312, -0.02757695,  0.04359658, -0.03537921],\n",
       "         [-0.03799623,  0.00689935,  0.02657298, -0.04832831],\n",
       "         [ 0.019558  ,  0.00797454, -0.02867308,  0.01105939],\n",
       "         [ 0.02977253,  0.02150572, -0.02371547,  0.02535402],\n",
       "         [ 0.0018843 ,  0.0083848 , -0.01313395, -0.04100952],\n",
       "         [-0.03851343, -0.00084137,  0.00652003,  0.03005591],\n",
       "         [-0.0193126 , -0.02821339, -0.03997569, -0.0069155 ],\n",
       "         [-0.00649624, -0.02425881, -0.04920071,  0.04663258]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       " array([[[-0.02570603, -0.0425355 ,  0.00116561, -0.04027306],\n",
       "         [-0.03712282,  0.00933958,  0.00916104,  0.01492542],\n",
       "         [-0.00481151,  0.00255662,  0.04010916,  0.04437927],\n",
       "         [ 0.03266637,  0.00223578,  0.01528454,  0.02182579],\n",
       "         [-0.0134742 , -0.02455674, -0.03205824,  0.03517307],\n",
       "         [-0.0286466 , -0.03366031, -0.02564389,  0.00422225],\n",
       "         [-0.03515984, -0.00892315, -0.0419554 , -0.04400741],\n",
       "         [ 0.04022003, -0.01787087,  0.00071216, -0.01416185],\n",
       "         [ 0.04022003, -0.01787087,  0.00071216, -0.01416185],\n",
       "         [ 0.04022003, -0.01787087,  0.00071216, -0.01416185]],\n",
       " \n",
       "        [[-0.02570603, -0.0425355 ,  0.00116561, -0.04027306],\n",
       "         [ 0.03513073, -0.04301738,  0.00929197,  0.02501892],\n",
       "         [ 0.0065163 ,  0.018814  ,  0.00966164,  0.00252904],\n",
       "         [-0.00698811,  0.04332395, -0.00371997, -0.03929681],\n",
       "         [ 0.01060241, -0.00249513,  0.04607243, -0.01014801],\n",
       "         [-0.02585864, -0.03934548,  0.02350101,  0.00471745],\n",
       "         [ 0.03266637,  0.00223578,  0.01528454,  0.02182579],\n",
       "         [ 0.00265933, -0.04623572, -0.04255412, -0.02477479],\n",
       "         [-0.01967419,  0.00610356,  0.01756929, -0.01711733],\n",
       "         [-0.03515984, -0.00892315, -0.0419554 , -0.04400741]]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## First! do word embedding\n",
    "\n",
    "vocab_size_en = subword_encoder_en.vocab_size + 2 ## plus BOS, EOS\n",
    "vocab_size_zh = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# to 4 dim\n",
    "## random vectors for before-training\n",
    "d_model = 4\n",
    "embedding_layer_en = tf.keras.layers.Embedding(vocab_size_en, d_model)\n",
    "embedding_layer_zh = tf.keras.layers.Embedding(vocab_size_zh, d_model)\n",
    "\n",
    "emb_inp = embedding_layer_en(inp)\n",
    "emb_tar = embedding_layer_zh(tar)\n",
    "emb_inp, emb_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1, 8), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0., 0., 0., 0., 0., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0., 0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Masking\n",
    "## include two part\n",
    "## one is inside the decoder, another is for padding\n",
    "## mask the padding because it's meaningless\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] ## we will use it after, for broadcasting\n",
    "\n",
    "inp_mask = create_padding_mask(inp)\n",
    "inp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8113  125 1558    8 1166 7960 8114    0]\n",
      " [8113   16 4111 6735   12 2750 7903 8114]], shape=(2, 8), dtype=int64)\n",
      "\n",
      "tf.squeeze(inp_mask): tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(2, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"inp:\", inp)\n",
    "print()\n",
    "print(\"tf.squeeze(inp_mask):\", tf.squeeze(inp_mask))\n",
    "\n",
    "## we can just mask the 1 part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"scale_dot.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale dot attention\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True) ## matrix mutiplication\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32) ## len of seq\n",
    "    \n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) \n",
    "    \n",
    "    ## masking\n",
    "    if mask is not None: ## which means has 1\n",
    "        scaled_attention_logits += (mask * -1e9) \n",
    "        ## paper denote as negative infinite\n",
    "        ## we just add a really big negative number\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "      \n",
    "    output = tf.matmul(attention_weights, v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## decoder mask\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1-tf.linalg.band_part(tf.ones((size, size)), -1, 0) ##Upper right triangle has value\n",
    "    return mask\n",
    "\n",
    "seq_len = emb_tar.shape[1]\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "print(look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights: tf.Tensor(\n",
      "[[[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.4997743  0.50022566 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.3328728  0.3333655  0.3337617  0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.24975398 0.24988763 0.25015718 0.2502012  0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.1999227  0.1999792  0.1999567  0.19990727 0.20023407 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16675764 0.16664171 0.16652788 0.16648448 0.16677557 0.16681276\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.14302464 0.14282268 0.14256084 0.14261211 0.14284477 0.14296679\n",
      "   0.14316817 0.         0.         0.        ]\n",
      "  [0.125028   0.12489291 0.12495697 0.12507041 0.12497041 0.12497012\n",
      "   0.12496808 0.12514313 0.         0.        ]\n",
      "  [0.11112186 0.1110018  0.11105873 0.11115956 0.11107069 0.11107042\n",
      "   0.11106861 0.11122418 0.11122418 0.        ]\n",
      "  [0.09999949 0.09989145 0.09994269 0.10003342 0.09995344 0.09995321\n",
      "   0.09995157 0.10009158 0.10009158 0.10009158]]\n",
      "\n",
      " [[1.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.49951664 0.5004834  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.3332119  0.33331707 0.333471   0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.24996088 0.24958098 0.25005078 0.2504073  0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.19995396 0.19998977 0.19996852 0.1999289  0.20015885 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.16678573 0.16669776 0.16654862 0.16645479 0.16667597 0.16683708\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.1427137  0.1429662  0.1428746  0.14276722 0.14290074 0.1428083\n",
      "   0.14296924 0.         0.         0.        ]\n",
      "  [0.12514567 0.12503438 0.12488484 0.1249121  0.1248699  0.12500727\n",
      "   0.12489225 0.12525353 0.         0.        ]\n",
      "  [0.11115278 0.11103198 0.11110595 0.11115574 0.11114185 0.11113304\n",
      "   0.11105889 0.11106312 0.11115667 0.        ]\n",
      "  [0.10012125 0.09985384 0.09992518 0.10005809 0.09987906 0.09997418\n",
      "   0.09983247 0.10013066 0.10000351 0.10022172]]], shape=(2, 10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = temp_k = emb_tar\n",
    "\n",
    "temp_v = tf.cast(tf.math.greater(\n",
    "    tf.random.uniform(shape=emb_tar.shape), 0.5), tf.float32)\n",
    "\n",
    "_, attention_weights = scaled_dot_product_attention(\n",
    "    temp_q, temp_k, temp_v, look_ahead_mask)\n",
    "\n",
    "print(\"attention_weights:\", attention_weights)\n",
    "\n",
    "## print out! we can see that in a sentence, the word only attention words before\n",
    "## 我畫個方格圖讓大家了解！\n",
    "## row col 都是 我是一隻豬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mutihead.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "## multi-head attetion\n",
    "## just like considering different aspects, like grammar, syntax.....\n",
    "\n",
    "def split_heads(x, d_model, num_heads):\n",
    "    ## x.shape: (batch_size, seq_len, d_model)\n",
    "    batch_size = tf.shape(x)[0]\n",
    "  \n",
    "    assert d_model % num_heads == 0 ## ensure the division\n",
    "    depth = d_model // num_heads  ## dim of each head\n",
    "    \n",
    "    ## num_heads * depth = d_model, split the origin \n",
    "    reshaped_x = tf.reshape(x, shape=(batch_size, -1, num_heads, depth))\n",
    "    \n",
    "    ## change dim\n",
    "    ## (batch_size, num_heads, seq_len, depth)\n",
    "    output = tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tf.Tensor(\n",
      "[[[[-0.03639312 -0.02757695]\n",
      "   [ 0.01971773 -0.0400288 ]\n",
      "   [ 0.02722507 -0.02708452]\n",
      "   [-0.03032577 -0.0354887 ]\n",
      "   [ 0.01879238  0.02930457]\n",
      "   [ 0.03148792 -0.02128877]\n",
      "   [-0.00649624 -0.02425881]\n",
      "   [ 0.04220078  0.02385452]]\n",
      "\n",
      "  [[ 0.04359658 -0.03537921]\n",
      "   [-0.03068806  0.00325962]\n",
      "   [ 0.03049209 -0.02288125]\n",
      "   [ 0.03697484  0.02999594]\n",
      "   [ 0.04095887 -0.04010413]\n",
      "   [-0.02220495  0.01309374]\n",
      "   [-0.04920071  0.04663258]\n",
      "   [-0.04670385  0.04206574]]]\n",
      "\n",
      "\n",
      " [[[-0.03639312 -0.02757695]\n",
      "   [-0.03799623  0.00689935]\n",
      "   [ 0.019558    0.00797454]\n",
      "   [ 0.02977253  0.02150572]\n",
      "   [ 0.0018843   0.0083848 ]\n",
      "   [-0.03851343 -0.00084137]\n",
      "   [-0.0193126  -0.02821339]\n",
      "   [-0.00649624 -0.02425881]]\n",
      "\n",
      "  [[ 0.04359658 -0.03537921]\n",
      "   [ 0.02657298 -0.04832831]\n",
      "   [-0.02867308  0.01105939]\n",
      "   [-0.02371547  0.02535402]\n",
      "   [-0.01313395 -0.04100952]\n",
      "   [ 0.00652003  0.03005591]\n",
      "   [-0.03997569 -0.0069155 ]\n",
      "   [-0.04920071  0.04663258]]]], shape=(2, 2, 8, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## take a peak!\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "x = emb_inp\n",
    "\n",
    "output = split_heads(x, d_model, num_heads)  \n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    ## some variables we r going to use\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads \n",
    "        self.d_model = d_model\n",
    "    \n",
    "        assert d_model % self.num_heads == 0 \n",
    "    \n",
    "        self.depth = d_model // self.num_heads \n",
    "    \n",
    "        ## the linear transformation for qkv\n",
    "        self.wq = tf.keras.layers.Dense(d_model)  \n",
    "        self.wk = tf.keras.layers.Dense(d_model)  \n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model) \n",
    "  \n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "  \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)  \n",
    "        k = self.wk(k) \n",
    "        v = self.wv(v)  \n",
    "\n",
    "        q = self.split_heads(q, batch_size)  \n",
    "        k = self.split_heads(k, batch_size)  \n",
    "        v = self.split_heads(v, batch_size)  \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        ## concat, combining the heads\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) \n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model_all.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feed forward network\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    \n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model) \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder layer\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  ## dropout rate = 0.1\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, attn = self.mha(x, x, x, mask)  \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(x + attn_output)  \n",
    "\n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training)  # 記得 training\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decoder\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    ## two types of mask are used\n",
    "    def call(self, x, enc_output, training, combined_mask, inp_padding_mask):\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, inp_padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "    \n",
    "        ffn_output = self.ffn(out2)\n",
    "\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2) \n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    ## ouput multihead weitgh for guther analization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"position.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "## position encoding\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "seq_len = 50\n",
    "d_model = 512\n",
    "\n",
    "pos_encoding = positional_encoding(seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAELCAYAAAA1AlaNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABdM0lEQVR4nO29eZxdZZ3n//6e5e61L9n3BEIgECRsgooICq6ouLUL2Dra81PbpRd1/I3OONMztt2/sW27p1vaBdoVxA0FZRPQFhESCAQI2ROSSiWV1F5113PO9/fHObfqVqUqdZNUVaqS583r4Z7znHPveU7dm+c+97t8vqKqGAwGg+HMwDrVAzAYDAbD9GEmfYPBYDiDMJO+wWAwnEGYSd9gMBjOIMykbzAYDGcQZtI3GAyGM4gpnfRFZI+IbBaRTSKyIeprFJH7RWR79NgwlWMwGAyGU4WIfEtEOkTk2XGOi4j8o4jsEJFnROQlFcduiubJ7SJy02SNaTpW+q9U1XWquj7a/wzwoKquAh6M9g0Gg+F05FbgumMcvx5YFbUPAf8C4eIY+AJwKXAJ8IXJWiCfCvPOm4Dbou3bgBtOwRgMBoNhylHV3wJdxzjlTcC/a8hjQL2IzANeA9yvql2q2g3cz7G/PKrGmYwXOQYK3CciCnxdVW8B5qhqe3T8IDBnrCeKyIcIv/lIp+QiK1vPqvMH2ZZvwHpRWHVWN9t2NuGlLdbMPcyWthacviLxlR5znH52HpqD0zGIxGMUFtickzlMf+Cwv7uJ+JESWihCTYpCg9Ba00eLncdCGFTlUKmWwcEEThacrA+FIhoEiCXguGjMQR3Bj4EfA9yAmOuTtEskrSJx8YiJjyNgIdF/5T+IokCAEqjiI/gIgVr4WPgqKIKPRaAWARL2qRAgBBXbYTJ1+KjRFYa2dfgNIDpvuGPUOzTeO0fF8471Dp8C7LiPn7dBwCpCXdMguRcs8guSLKrr5PBzSfJLkqyu7WDX3lasbAl7ubIk3sPzXXNIHMihQYDXmqappZc5dpH9XorerjSx7vDzIYk4xVoHqfWZl+glKSUSYlEioNtP0FVMUcq72HmwC4pV8MHzUN8fGqdYFtg2ODbqWASOENigFQ1RsBXLUmwrwLECHAmwZfjRIsAWJfxEgCUBVvSuh00RYcRnrfKdk4q9ke/o0e/vBO/4jGfjM4Ujqtpyos9/zSvT2tnlT3xieK3ngHxF1y3RPFctC4B9Ffv7o77x+k+aqZ70r1TVNhFpBe4XkRcqD6qqRl8IRxH94W4BWH9BQhs2v4pf/nojr3n+LSQ/6nL3vT/mNW99Hx3r0zz+2X/h0v/yZzTft4dl3+3ik62/4cYvf4rWf3oUZ+FSdvzvOh654hYeyDXx6Tvex8pvHsDfuw/v0nXseqvDR19xP/+5/gWSEmdjscjft13H4xtW0bJRaNzUA7v24Q8MYCWSWHNaKS5poljv0rfEZmChovPzLJ7TxZqGg5yXbmNV7CCLnV6abYuUuLhiY0U/qkrq4eGTVY/BIKBfbfqDGP1Bgr4gSb+fJK8uA36Cfj9BLogx4MXI+THyvkved8j7DkXfpug7BCp4vkUpsAmCcDsIrPDLIRCCIHzUIPpSCGT4SyAANPry0HAbwjloaDLXii+Lyndq9PGIo97Nk/xSGPvTAbUreujf0kAQV1L7LV73J4/yzMsybPvL8/jy677L1887j61fOI97r/0qb/vIx8k82UbmWwW+ufRu1t3+CVb916fRQoFD77qUmz/8Kz7esIfPHLqAX97xUpbc0Y63czf2yrNou7YZ+1VdfPbsX7Mu3sYqN8NBf4Cf9K/mB/su5sALrdRttWjYXiSxpxvtOILf1w8aILaNlUohdbVoYy3FphT5Jpd8g0WhHkq1UKoN0HiAlSmRTBepT+VoTGRpjGdpjA3S4GSpc7LUWHlq7Bxpq4CLT9oqkBAPVwJiBLiiuAIugi2ChWATPgJRX/gZtCqmdVuO/rFvzfL4Dnve9r0n8/zOLp/H711c7bXyFabrWcGUvruq2hY9dgA/JbRNHYp+vhA9dkzlGAwGg+F4CH+JV/ffJNAGLKrYXxj1jdd/0kzZpC8iaRGpKW8DrwaeBe4Cyp7om4CfT9UYDAaD4XhRlJL6VbVJ4C7gfVEUz2VAb2T+vhd4tYg0RA7cV0d9J81UmnfmAD8VkfJ1vq+qvxaRJ4A7ROQDwF7g7VM4BoPBYDhuJmkVj4j8ALgKaBaR/YQROS6Aqv4rcA/wWmAHkAXeHx3rEpH/ATwRvdQXVfVYDuGqmbJJX1V3AReM0d8JvOp4XqvdS9Bw+QVsKm5g4PvzOXSj8OtcDHniefQzK7k/59D8+w4GL1rEB5t/xh29L6H5mRx2JkNuVQsvXbKVjJXggd5zqXkR9HAnVibD4LwYidYB1iTaSEqcknrsKTXxYn89bq9FvNdHBnIExSIAEouhyThe0qaUErwEBAkllvBIu0VqnTw1Vmh3jYsO2VRH20h9VXxVSpFj1o+cuCV1KKkd9kXHSkHk0FUZ0VTLjtthp65G/WXnrVbY6Yn6xtyuYEwbepV2+umy5wM0fznJ/7r1W/zjm27g7Nt2ctfPXkrTq31uvvoRPvPUW1jGNm5e/yi3dK8ns3E/gxcu5GPzbuM/CrU0PC8E2SzOvLn0rQi4Mr2N7sDj8SNLSB9QtLMbKxaj1JSm0Azn1nexyO2k2Q7fx8O+zf5iI92DSZwBC3dQsXMeFAposQgaTRhigetAzCWIOQQxi8CFwAV1yo+KOorlBLiOj2v5xOzw0RUf1/LCR/GwCcImwxOSjWJJ+JPdHscFa8tsd81OL0r473NSXkv1XRMcV+Aj4xz7FvCtSRlIBVPtyDUYDIZZR3CqwtKmATPpGwwGQwUK+GbSNxgMhjMHs9I3GAyGMwQFSqdxGdlZkYXR1VnD9g+7vOv3/4nmnzzPy960iU9sfAdWOsXfnPszvrjjDXg7dnPwUofzYzG+v2M9sa1tsGQ+Xee4vL5pEwNBnkfbl1Gzt4Q/MIg0NTA4V1jRfIQVTugUH9AiuwutHOnNEO+BWG8JcnmCkofYNhKPoUkXL2XhJQU/CUEiIBErURvLU2Pnh5JmYiK4Yo1IfgkjezVqECgUsSlhU1IHX2XYoRs4I7JyPbXwKh26HO3UDXQch91RDtwqs2zHe70ZgPzuKV6ZGMR/bjv/37wnWHb7YQ7eWOSvmjZTd3ca/5Jz+EjjBr696XK8tgMcWu/wikQ/3+u4nMbnB7HiCbylc6hb3sPZTsCOksueA81kDpQIBgaRTJp8S5xis8eKzBHm2zkyEqOgJTr8DG25enIDcdxBQkdutgSFIlRm49o24jjgOqhrEcSEwJUKZ66Co4ij2E4QOm9tH0cCYpED10Yj563iio+FRn0jWyWjE7MMx4ei+FW22YhZ6RsMBkMlCv7snM+rwkz6BoPBUEGYkXv6YiZ9g8FgGEGYJ3O6Mits+rHDOX5y1T+z5FYbfJ+/n/8QjT9LM3jVal6XytP9yFysZIK5lx5gICjgb6rD6zhC3zmN9K32uCLRzrMlh97d9ST394EGeHPqyM5T1tYfYI5tU9ASh3xlW3YOpe44sT5w+otoNjcknkUijp8atul7CZC4TyZepMYpkBmy6fu4kcDV6MSsgIASAcVIYbOcnFVUeyg5y0ciO364XQrs4QStUbb8sr+pvD2UoDXqONGxEVSKrZ0Ip9Dm3/8nl/PaLW+h8Lr1/D5v4b2wna9d+n0O+Xma79/DvmuS1Eqcht8nsGtqSK/vJC4uv9+6EmfnAawFc+k5K8UrF2wnYyV4LLeC2L448fZ+1Csh9XVkWyzizTlWJw9Qbzm44jCgRQ54DRzM1aADLs4guIM+kguVOYcUNsVCXAdcF3VtgpiN7wq+KwQOBE6FTd8OQnVNOyBm+UP2/KEErXJyVoXipiWKVZG9Zkv5cez3pFqxNUO00tfq2mzErPQNBoOhAgWKs2M9fEKYSd9gMBhGMW4k3GmAmfQNBoOhgjAj9/Sd9GfHbxjbYqUDzoNP0vXmtfRqifpfv8C+1yp7vH4WPDRIcMEqPrH0Ae7JzqflqTCuvmuNxaqzDjDXzvDQwBoyeyyk/QhWMkl2fhLm5rkwtZeMFac7KLDPq2N3fyNOj0O8J8AayKP5AhCKrRGP4ycdSpE9P0gGuAmPTKxArZunxsqTkiIJCXCxjoqT9jUYElsLgFLZlo89FI8fqEVJbUqBja+CF9hjiq3pqHj9IQG26H+VuSVH2/I52o6vMlLgrLJ4yliPU8yxxNYAzv/E03hfm0ffh/u46bcfQNav5bpkkY/tvhHvQDvnvWo7d2fraP19J/55y/ngit/zXLFAzTMxvMOd5FY207tKuLbuWQpa4rddZ5FuAzncjdg2fkstuVZY2NDD8lgHGSsOQJev7C000zGQwem3cAfC6mqSK4yomiW2DU5k0y+LrcVkpOCao2AHw2Jrdii25lg+rhWMtOVHYmsufsX+sNgaTM4/5tleQGUyKFeuq6bNRsxK32AwGEZhzDsGg8FwhqAIRbVP9TCmDDPpGwwGQwVhctbsNN1Ug5n0DQaDYRTGkXuKKbTEefPWG3FWLsN+dwd/uv2dBH0DfPTKB/hC2+uwntzKgSvTXJ/q4Za9LyPzzCHsxQvw1gzy5nmbKGiJew+eQ91uH7+3D6uxgYEFFotbu1gVO4SFRZsfY3thLgd66oh3Q7zXg4Es6pXCRJt4nCAVx0vZ+EmGxdbiJWrdArV2jho7R8LycCUSvTqG2FqpXB0rElgrqk1xqHJW6CTyAjt01DLssPUCq8J5G762KiOcuUOoDDleh49NzYd5sqtmTcS/LvwDiZ//kXvWfZMV3w7Y8ScZfjaYZvcvlmOfexb/Y9HP+eILryd4YRcHL0vzlsxWvtd9Gc2bi4gldJ8dg7MGeEmsi91ekWfa5lOzzyPo6cNKpci3Jig0B5xd18ECux8Li5J6HPQz7M83MDCQxBkQYoMB9mBpuGoWkdCabUfJWc5Q1SzfFYIYkTM3TMyynADbCROzXMsnZvnELQ9HKhOzfGzRUGxNgjFF1spVs0aLrZmqWcePlqvZVdFmI7Nz1AaDwTCFjFhsHaNVg4hcJyJbRWSHiHxmjONfEZFNUdsmIj0Vx/yKY3dNxr0Z847BYDBUEMbpT856WERs4J+Ba4H9wBMicpeqPj90PdVPVpz/MeDCipfIqeq6SRlMhJn0DQaDoQJFKOmkTY2XADtUdReAiPwQeBPw/Djnvwv4wmRdfCxmhXlnQWMX2X9bwM6b5vCDNbdx+M7F+Fes5SP12/jDw+eivo91RQ8A+5+cj79vP9lzWrlmxVauTW9lW8lj7+5W0ntCMa1gbiOD8+CCxjYWOT4l9dhVbGFrdh7Z7iTxHnB7C+hgNnxt14FEnCDlUkpbeEnwkkAiIB0vUhfLjSig4iK4Yo8ptjZcgEEoqU1RwySsgFBQrdxfFlsbkaA1SmwtYDhRC44ttjbisbw7lt29Wlt8hf9gokSqqeCT7Rchl55PXhXrkSf51HW/5C/+8HYW/fwQ+69rZrWbxnu4EbFtCpcP0Gpn+Om2C0hsaceeO4fes32uWLKLVjvDo7nlBC+mSLYNEBTySEMdg3Mc7NYca1IHaLbD93FAi7R5DezL1uP3ubj94A4EWLkCWiiMEFvDtsLErHhkzy8nZjlhYpa6ipZt+laAawXDiVkSkLBKkS0/SsoaStLSIbG1oSIq44itHcueb8TWjo2vUlWrggXAvor9/VHfUYjIEmAZ8JuK7oSIbBCRx0TkhhO8nRGYlb7BYDBUUM7IrZJmEdlQsX+Lqt5ygpd+J3CnqvoVfUtUtU1ElgO/EZHNqrrzBF8fMJO+wWAwHEVQfWTOEVVdf4zjbcCiiv2FUd9YvBP4SGWHqrZFj7tE5GFCe/9JTfrmN57BYDBUUHbkTpL2zhPAKhFZJiIxwon9qCgcEVkNNAB/qOhrEJF4tN0MXMH4voCqmRWTfr3lUXPnRj705l9TY9nM+9ludr85Rm9QYPF9BWTtKj65+kEeyNXQ+kRoYD681uGtjRtY5qS4f3ANmR0O1oEjWPEE2YVpvPkFXpLZS4OVpDvIszU/j239LThdodia3ZdHCxVia8kEXtqllBK8FPhJxUmUqInnqXHy1Ng50mWxNRlfbK2kASUdKbZWLp4yVBBdrdDOX2HP9wLrKLG1yseRxVKGrzsUn19p11dGCqqNFd8/+jmVj1NMNT6CP3z1Ynb8uc31j/9n7HPP4v+p38/CH7t423fScv1+HszZLHi4D85dwfvX/IHtpQHcp9L47QcprprLwrM6eEPjJkrq8VD3ajIvCtbBLhCLoKWe7BxY2NTDqvhBMhIjpwW6goC9xSYODdRg99thUfQBD8kVoVgaFltzHcR1o4LoUQGVmAzb9V0IbJAoRt91fGKON1RAJW57Q2JrQ6JrDNvzj1dsbawCKsc670xHqc6eX41NX1U94KPAvcAW4A5VfU5Evigib6w49Z3AD1VHeNrOATaIyNPAQ8CXKqN+ThRj3jEYDIYKVJnM6B1U9R7gnlF9nx+1/9/GeN6jwNpJG0iEmfQNBoNhBNUnXs1GzKRvMBgMFSjMWomFajCTvsFgMIxithZIqYZZcWc7C3XYixfwqYbd/OmuG/APdXDz1Y/w+fZrcf+4hQNX1fOOmn3844uvou6pQ9gL55Nfm+PS+CC+Btzdfh71OwP8zm6spgb6Ftksnt/J2vh+LCwO+A5bBuayr7ueeJcQ7/agfzAU0IrE1jQVx0vbeCnwkkqQDEgmStTH8tQ5ubBqllUkIYqLdUyxNZ9hsTU/cugWh5y5YfOCMEGrUmxtZKUsRj3KSD/rcYqtTWqC1TQ4fOu+80d+87KvMf/rcXa8r4n7cw6p+zfjnL2Sf1h5B5/b+mb06a20v6yOm+o28e/dl9H6VAmAznMTvHH+Zi5LHGKPl+eJ/Yup3esRdPVgp1Pk5qXItwasrj/EUqcHVxx6gxIHvRR7c0309qVw+4V4f4CdLUG+gJa8cGBiIY6DxFyIxQjiLkHcGunEdRXcsGKWY0dVs6oQW7MIACYUWzsRjBN3GK389zZBm42Ylb7BYDBUoEyuI3emcfremcFgMJwQclrr6ZtJ32AwGCpQjisjd9YxK+7M64iz4wPz+eFAAy/+YAX+yy7gr5o289AD69BiCbmqGwuL3Y8vxt/zIoNr5/Hqs7aQsRK8UCqxe+dcMrv6UK+EP7+ZwYVwUdO+IbG17cU57OxpZrArRaIbYj1Hi6356diQ2JqfVEh6Q2JrdeUCKuKREGtCsbWSDoutldQeIbYWqIwrtlaZoBWKrVG92NqolcuEYmvHssufYrE1AC5dSwnBuX8Dn3rjL/jwf7wPsSz2vaGVtbEkuQdaENsmf8UAc+0Mt7/wEpLPtmHPnUPPOT6vyTzLXDvD73Ir8HenSe2NxNaaGhic62DPyXFeum1IbO2wb7PPa2LvYGMottYXia0NFtB8Piy2Q1hABdcZW2wtauoquKHYWmjPD8XWYrZ3TLG1mPjYBBOKrYUFfMp2/qP/iRuxtYkp+90marORKX/3RcQWkadE5JfR/jIR+WNUUOD2KDXZYDAYZgRhprtVVZuNTMeoP06Yflzmb4GvqOpKoBv4wDSMwWAwGKrGlEs8QURkIfA64BvRvgBXA3dGp9wG3DCVYzAYDIbjQaOaFtW02chUf1X9A/DXEAUYQxPQE4kQwbELCnwoKh6wwe/u4rNv/TGf+9XbmXvnNna+3eWQn2fJPVm46Bw+d86v+GW2kbmP+SAWh9c5vKvpjxz0B7hnYC2ZbQ7W/g6sZJLBxSmChTkuzuymwUrSGeR5NreQw501uEdcEp0Bdm8OzeXDccRikEriZYbF1kj5uEmPukSOejdHnZ2lRvKkxcdlpNhaQHBcYmulqKiKF9gTiq1V2u8VJldsjYrnVD5OMdX6CA582uPVD38M6yXn8md1bSz9vkX/q89l2Rt28YtsigX3dxFceDYfOe8RnitlSTyRxjtwkPw581m2up2zXIeClrj3yHnU7BGsg0dALPw59QzOg0XN3Zwdb6fOSpDTAi96DewutNDeX4vdZxPrB7ffQwYLY4utxWJozCGI2/hxiQTXIru+o2AHOE5ky3c8ErZXpdiaHpfYWpmJxNYMw4SO3NM3Tn/KJn0ReT3QoaobT+T5qnqLqq5X1fUu8UkencFgMIzPJEorzzimMmTzCuCNIvJaIAHUAl8F6kXEiVb7xyooYDAYDNNOOSP3dGXKvqpU9bOqulBVlxJqRf9GVd9NqAt9Y3TaTcDPp2oMBoPBcCIEWFW12cipGPWngU+JyA5CG/83T8EYDAaDYUxUJ7Uw+oxjWiZ9VX1YVV8fbe9S1UtUdaWqvk1VCxM9X5IJbq7t4Kxb+wj6Bvj81T/jY7tvxHrsOfZdm+GGdCdf3vYaajbsx162GF3XzyXxEg/nFvHzfRfQsN3D7+rGammib7HNivmHWRtvw8Jinxfn+f55SGeMeCckukvQP0hQFltLJggycUppm1IavJRiJTySiSL1sTy1UWJW2iriCrgyUmzN15FiayUVigw7cccTWxsSWisnbAXWGGJrw07dkX/wscTWjvH3LTt4J4Npcvj+4eJvseofS2z9cJpv9c0h9uAm2t9a5GvL7uSvN70F/9lttF2V5j21L/Avh69izoY8YtscWRvjbQueJC4u20oeG/cuom5XiaC7B7u2huz8FPm5Puc3HGCp24uFRVdQZFexlR3ZFvp6k8T6hHhfgDNQhEIhFOYjTMwSx4F4DFyHIO7gl8XWYpVia4rlDlfMitvekNhaXEpjiq2VHbpW5Okui66NFlurdNhWK6JmxNZGogwHUkzUZiNGhsFgMBhGMVuzbavBTPoGg8FQQTlk83TF/K4zGAyGEUyuDIOIXCciWyPpmc+McfxmETksIpui9sGKYzeJyPao3TQZdzcrJv38HIvPHLqA4Knn6X/DOm6u7WD3L5ZjJRMsftVeBrTIwKMtePvb6LmohXeetRFXbH50aD0dW5tJb+9BfR9vYTODS5TLmvawyBZyWuC5wgJ2dDURP2KR7FTc7jw6mAUNsGIxSCbwM3FKGQsvDX4qIJEsUpMoUOfmaHQGqLVyJMQnIRbl/8qEtvyAEgFF1SFRtXLxlNIoe35ZeK00ZDcctuWPFlsDhkTXJhRbG+qLjh9TUO1Yx2aA2BrwUL4R3fgct137b/zvX74Zu6WJr176Q+bYCWruyWDX1VL7ig7qrAR3bzqf2LN7sZcuov/8ItelX+BFr597B87D2ZkkubeXoFhEWpoYmG+TnDvIBel9zLEcAgIOeAl25Vp4caCBoDdGrA/cfj8UW8vlhxOzbDtM5ovF0IQbJmbFosSssuBaLCygYjsji6fELI+45eFalQVUPFzxIqE1HdEA7ChBq1JsjTH2K+38RmytOkb41I7RJkJEbOCfgeuBNcC7RGTNGKferqrrolZWMGgEvgBcClwCfEFEGk723swnwGAwGCpQhVJgV9Wq4BJgRxTAUgR+CLypyqG8BrhfVbtUtRu4H7juhG6qAjPpGwwGQwXHWS6xuSwXE7UPjXq5BcC+iv3xpGfeKiLPiMidIrLoOJ97XBhHrsFgMIyiGtNNxBFVXX+Sl/sF8ANVLYjIhwmFKK8+ydccl1mx0l9V08GvvvNSuOwCBt7by4M5m0U/P0T2Fefwv5f9hH/tXsf83+WwMxk61gtvr9vAtlKOp7Ytoe4FC/a3Y9fV0bc8SWzRAJdntlNrJTnkF3l6cBG9RzIkOiHZGWD1DKK5HACSiKPpFKW0QzETia2lfWqTBerjORpjg6HYmpUnJcGYYmuh4JqGgmvIOGJrw/b8oTj9itWEF1hjiq2NKIheFlurKKpC1D/EaBu8yki7/MmKrZ2kjf94fAR//d2byL75Yi6Jl1h1WzcHblzO61J5/q5zLS337iV75Vl8/qxfcF8uQfNjLl5nJ33nt3DF6h0sdWr4XW4J97SfS91OhfYOrFiM4vw6svPhrJbDrI4fIGMlGAgK7Ck1s2ugmfbeWtwem1iv4g6UIJtDC4XQpi8WRDH6GncI4i7+kNgaYZx+DNRRxAlwHJ+Y7ZNwPBJRrH7c8ohbpSFbfii0FuV+yrDgWlls7USLoBuOzSQLrrUBiyr2j5KeUdXOinylbwAXVfvcE2FWTPoGg8EwnUxi9M4TwKqoeFSMUJLmrsoTRGRexe4bGa4/ci/wahFpiBy4r476Tgpj3jEYDIZKJlE2WVU9Efko4WRtA99S1edE5IvABlW9C/hzEXkj4AFdwM3Rc7tE5H8QfnEAfFFVu052TGbSNxgMhgoU8CaxKpaq3gPcM6rv8xXbnwU+O85zvwV8a9IGg5n0DQaDYQQmI3cGYAssvG0r229K8KN13+DDj70Xb/tOXnydsC6W4N82vgz3qR34561g6YX7We2m+WnfhdQ859K4JY/f34/Ma6VvqXDRgn2scTsJCNheauDZnnk4HTGShwPinQXoHyAoeWGiTSpJUBOnVBOJraUVN1miLpGjMZ6l0Rmk1s6TlhIJkTHF1kIHbkAJJVBGiK2NKL0WDIutldTCq2hBlJDlBxb+CGdueJ3KxKywg8ixO3J/XMZy9o51/gz6h7DsH7eQ+mgbf7LrtfibX2Dp23byi2yKWx+8Cq/tAPuutXh1Ms+Xd11Hy2Od2HV1HL7Q5t2tf6CgJX52+CXs2TmHup05/N4+rIZ6BhcmKC4oclH9iyx1wspph3yfHYU57OurJ9ubJNZLKLbWV4BcHi2FReBGJGbFXYJE5MQd4chVNBaMEFsrV8waSs4Sn4RVIjZCdC2oqJ4Viq3Z0Vthi1QttjZWYpYRWxub07lyllnpGwwGQwWnexEVM+kbDAbDKI4jTn/WYSZ9g8FgqESNTf+Us3WgBfU8vnbtv7PQcVlwRwzn7JV8+GW/4bGCR8tDLn5/P4cuS/Nnix9hIMjzoz0X0vh8idiOg4jjkl1WT25ZiZfXb2OenaQ3yPN0bgkvHmkk2QHJI0Wc7ixBJLYm8TikknjpGMUawcuAn/ZJJQuhPT82SKMzQI2VI2X5uGLhYA/ZSMO0LB1qJYUiFiW1KKlNPnAje75DKXDwsYaKp5QLNJTthn5g4QfWCFt+MNqOH6GjbfmVK5ZKsbWxCq8cB5Mttna8ryfJJHed/TP2f2MF+rIL+cbyn/KJR9/Bsp8VcFav4oaXPUGHn6X90QXott34a5aSWtfFlfE+ninCxp2Lqdnm4O45DBoQzG2mf6Ewb143F6RepMlKUNASL3p1vDAwl97uFFa3S7wXYn0eMphH84VRYmsuxF2CuBOKrcUt/HhYQMUvJ2a5iu0ExCuKpyTs0pA9v9wsCYiJd5TQmo0O/aMd6x/vscTWDNWhgBdYVbXZiFnpGwwGQwXGpm8wGAxnGNWUGZ2tmEnfYDAYRmEcuQaDwXCGoMaRe+qJHYKD7zmX16Xy/NmL15K67xn2vaGVP294jv+y4y00/64dZ+kSspcN8prkYR7INTG4uZHUtsP4hw5jz2mhZ6XL0sUdXJzcjSsOuzyHjb1L8A4mSR5R3K4c0jOAFgogFlYySZBJUqp1KGWEUlqxUyUakjkaYlma3LLCZoGEKC5HJ2aVK2aVVCmpRE5ch7zGKKpNsSJBy9MwKasUOXNDh641onLWSAcuYytsViRsDTGW45ZjOE+Ppcw5HtNcRWvrXyzhvlwdjT98ih03O/iqLPyxi/37zey/voVPt/yWfzhyBfP/o4j6Ph3r07xn+eNkrAQ/7b2I1NY4Dds9gsOd2DU15BZnyC4MuLB5P6vdDlxx6A4KbC/OZXd/I3THiHUL8d4Ap7c4pLCJBsNO3Cgxy084+AkbLyH4cfDjFYlZjh8lZvkkbG/IiRu3PBJWiYSUcMUnJj4WGilu+lij3qyywuaxErMmcuKaxKzxGA6emKjNRsxK32AwGEZhbPoGg8FwhnC6a++YSd9gMBgqKZtLT1Nmh1FqMMdlNz/JFw6fy6bbz0ViMZa9YRdZ9eh4cAHerj10v3Q+7z/3MVJWjG8fuJKmZ5Vg/wHUK1FaPoe+lQFXzdnOSgdyWuDp/CK2HGkl0WGR7PCQnn60rx/1faxYDDIp/No4hVqLUgb8dEAyXaQhkaPJDROzaq1cJLZm4Yo9wkYaJmSF9vyiapSYZZNXJ6ycdZTgmoWndiiyFtihPb8stlaulEWFLT+y51eKrelRYmkywtY+lJhVSbX2+4qVz1G+gFPwD+QHb/4af/Gjm7CaG/m3q77N+3e+jdT9m7FqM9Rd306DleCOxy8hvnEn9spl9F5U4MaaZ3jR6+fuPefSsDUgvb2boJBH5rbSu8QhubCfS2t2Mc92CAjY58XZMjifQ921xLot4j0Q6/GxB/JoLk8wWmwtESdIuvgJC68stuZC4IY2fWIBTswn7noknBKJoWpZoT1/ODkrrJwVE39kUla5YtbQ48jV6Oj9kcdmxz/1mULZrzZRm42Ylb7BYDBUUF5Mna6YSd9gMBhGIPiBmfQNBoPhjOF0XunPCkOf1qX4pwV/4Ed3vpyFt++i+w1r+NqyO/nCwVey6P4+nKYmDl4Z8L76jWws+GzevIT6zd0ExSJOUxM9qxLULe/hlZktZKwE+70Sj/ctp+9gDckOSBzOob39BLmwcIYkE2hNilKtS7FGKGWAjEdtMk9LfIBmt59GZ5B6K0dKAuLYuGIPjTeI/vMJ7fklwhj9vLpRbP5Ie75XUUjFC+yjbPlhTPCw7b5ye0SMPozoH/4DykjHlMpIu/x4H/BpstWfiHjbIqfAqn/ey+4PLOWVyRLtdyxFLIue61bzlbPu4IcDc5n3kIXf1U3nJS284bzNLHZq+OXAOeSfr6dmWw/adhArniC/pIGBxcoFcw+wNr6fjJWgO8jxQnEeL/S1UupMEO+GRLcS6y3CQHYoRh+xhuz5mijH6Ft4SSuM0U9AEFfUVcQNcByfeFQ8JWGXSNrFyK5fIm6Vhmz5Lj4WAZYollQIrslwjP5YVBujbxifEX6zCVo1iMh1IrJVRHaIyGfGOP4pEXleRJ4RkQdFZEnFMV9ENkXtrtHPPRHMSt9gMBhGMVkhmyJiA/8MXAvsB54QkbtU9fmK054C1qtqVkT+M/Bl4B3RsZyqrpuUwUTMipW+wWAwTCeq1bUquATYoaq7VLUI/BB408hr6UOqmo12HwMWTua9jMZM+gaDwVCBIgSBVVUDmkVkQ0X70KiXWwDsq9jfH/WNxweAX1XsJ6LXfUxEbpiM+zPmHYPBYBjFcbiZjqjq+sm4poi8B1gPvKKie4mqtonIcuA3IrJZVXeezHWmbKUvIgkReVxEnhaR50Tkv0f9y0Tkj5FT43YRiU30Wtrq89PBOpZ9pw3/cCe5t/Uwx05w3wMXoZu2MHj5Cq67+GkW2DX82+FX0LTJht37sevrCZbOo+cs4RXzd7AmNkhJPTYVFrDpyALiBx3Shzys7gE0l0O9ElYshqRT+DUJCnU2pRoo1QQk0kWak4NhxSx7gHp7kJRVIiFyVGKWrwG+RslZSCS2ZkdJWaNaECVmRZWyyk7cMEkrTNAqJ2ZVOnCHhdYqErNGJF/JqP2x/rBjbB+V4MWMS8wCeOlv/hzt6eXD7/gVnzl0EfN+vIP+V59L1w2DXBSL8T+eei31v3sRZ8F8Oi4N+GDzb+kMBvnBvotpfF7hxXb8/n6suS30LY0hi7NcUb+DJU4AwAHP4rnsAvZ1NRDrskl0K/EeH7svh2azaLEIhIlZxFyIxwgSsVBorezEjUMQCx25xANs1yfm+mFiluORjMTWUnaRhHjEyk5c8bAliJKzgpGJWZGD1hYZIbZ2vIlZRmztGEyuI7cNWFSxvzDqG4GIXAN8DnijqhaGhqLaFj3uAh4GLjzxGwuZyne+AFytqhcA64DrROQy4G+Br6jqSqCb8OeMwWAwzBy0yjYxTwCrosVuDHgnMCIKR0QuBL5OOOF3VPQ3iEg82m4GrgAqHcAnxJRN+hoyEO26UVPgauDOqP824IapGoPBYDCcCJO10ldVD/gocC+wBbhDVZ8TkS+KyBuj0/4OyAA/GhWaeQ6wQUSeBh4CvjQq6ueEmFKbfhSutBFYSRi2tBPoif4QcAynRuQQ+RBArLV2KodpMBgMI5hMwTVVvQe4Z1Tf5yu2rxnneY8CaydvJCFTathTVT+KMV1IGLq0+jiee4uqrlfV9Wvm5vj0Xe/G37ef/PUXcusFt/J3nWtZck8OK5mk7SqbT7Y+yAulQR7YvIbmp/rwBwbQZfPpOacGd1Ufr6nfTJOVps3P8Wj/Sjra60m1Q/JQHu3pJSjbaJNJtDZDsS5GoVYo1QA1HvXpHM2JAea4fTQ5A9RbWWrEJyH2iESYgAAPPyyewnDxlKLalLDJa4x84JJXl0LgVtj3hwXWysVTyquJoGJlERy12mBEYla4Md4fleoTs6aJE0nMAlj9dwO0/+n5fLxhD/fcfhl+Zzftby3ytYt+wK9zMeruT+O1HaDvssVccdFW1saS3DO4hPZn51C/pR+/txcrFqO4pJn+pbB2QTsXJ3fTYKXoC3K8UJzLlr655LqSxDsh0e0T6yki/Vk0l0d9P0rMcpFEAk3G8FMOXtLCSzCygIqriOvjuj4JxwtbRfGUcmJWXMLkLJsgTM4SxZVgRGIWmJC7qUYVNLCqarORaYneUdUeEXkIuByoFxEnWu2P6dQwGAyGU4mRVj4BRKRFROqj7SRhRtoWQtvUjdFpNwE/n6oxGAwGwwkxeY7cGcdUrvTnAbdFdn2L0IHxSxF5HvihiPxPwvTjb07hGAwGg+E4qV5XZzYyZZO+qj7DGDGlUbzpJcfzWlm1OPuWw2Svu4iD781zfizGjQ9exarHNpK/6nxeesXzrHIzfPzAxTQ84SLbXsCuqaF7TR3dq4VrlmznJbEuSppgU2Eujx9eQqzNJdPuYx/pJxgYBEAcF8mk8eqSFBpsirVhjH48U6ApOciceD/NTl9oz7eKJESIi4Mrw3/GETH6SiS05pBXl/wIG34Yo18InKhoij2ieEr50Q8s/AqRtSAYWQx9dIz+CLt+ZMOvunjKLInRB9Btu3nj957jf3eexZLv7iX7mgv56qXf4dqkx3mP/QlLHmyDOa20vwz+z9wH6As8bt13OU2bQXa3IY6L1dpC74oE3rIcL2/cznK3CLjs85VnsovY2dlE7LATCq11F3F6cjCYGxGjL4l4VDwljp908JIybM8vi63Fo+IpsbB4SsopkYzE1lJ2YVSMfthsCcaM0S8XTxkrRn8ssTUTo3+CzNJVfDVU9e6LyFtEZLuI9IpIn4j0i0jfVA/OYDAYpp1JVtmcaVS70v8y8AZV3TKVgzEYDIYZwSyd0Kuh2kn/kJnwDQbDGcNpbN6pdtLfICK3Az8jlFcAQFV/MhWDMhgMhlPKaTzpV+vRqQWywKuBN0Tt9VM1qNHs6WnB37Gb9pvzfO+Sb/K3natZ9rMCEoux71qXz87/FS+UBvnF0xfQsqEfv78fXbmI7nMEa3U/r2/YRKudoc3P8UjfatoPNJBuh+TBHNrdQ1Asho69dCpMzKqPUaizKNWC1nk0pHO0JvuHErNqrGJViVn5yIlbbWLWUKs6MUvGScwarxLW6ZGYBXDwQ+v57y3P8b3vvwr/YAf73lXidak89+cckvfU4e3ZS98Vy7j80he4OO7yi8EF7H1mAY2b+/G7urHntlJc3krfCmHtogNcmtpBk5WmL8jxfGEez/QuYPBImkQnJDt9nJ4c0jdIkM0eOzErOezEDQXXAqzYcGJW0i2FFbOsMDkrIR6ueEOJWTHxcPFxJRgzMetYwmqjGcuJa6iCcsJjNW0WUtVKX1XfP9UDMRgMhpnCGZ+cJSILReSnItIRtR+LyJRWdzEYDIZTRiDVtVlItb//vk0oBzo/ar+I+gwGg+G0Q7S6NhupdtJvUdVvq6oXtVuBlikc1wgSHSWyb1jPjy/5Oue7Ft/61dXYv99M7mXn8KpXPM1qN80/HLqGxsdcrBd2Y9fX0722DmdNL69ZtoWL492U1GNjYT5/OLSM+L4YmTYP+3AfGiVmWekUkknj1ycpNDoU66BUG5DIFGhJDYxMzBJ/wsSsfIXQ2nElZgXVJ2aNSLwaSsyqKJ4y4njFqmSWJ2YBvPvP7uO/dJzPklt3kX39S/jGS2/jF9kUH9v4Llp/vRdn/jzaXin8xbx76Q6yfH3vy2l+CmTHvlBobcUcelYl8Vdkubr5Bc52w4SrvR48lV3CjiPNxDsckkeUeGcR6RtEBwaPnZiVssLErOTIxCz7GIlZcatEwiqNSMwar3hK+R9rtYlZY2ESs6qgWgmG03zS7xSR94iIHbX3AJ1TOTCDwWA4NVTpxJ2ljtxqJ/0/Bd4OHATaCQXTjHPXYDCcnpzGK/1qo3f2Am+c8ESDwWA4HZilE3o1HHPSF5G/VtUvi8jXGOPPoKp/PmUjq6RYovihLpY7Nn/Zfjkr7hjAqs2w9/UW3513HxuLce57Yi2r/9iNPzCAXHo+nWvhrcue47q6Z2iy0mwvDfBgzxoO7W+gaT8kD+TQrjBG34rFkJoMWpem2BgnXy8U60BrSzRkssxJ9jMv1kOr00e9VSBtWcSxccUeGmJljH5edaTQmrpHxegXNLTxFwKHYuBQDOwRMfp+YI0ohD6iKHplARWoiNmvfHOOFlo7rhj9Kf7perJOsE81buei//UR5vZuovumAa5KKCsefAdzfhXH2/8Mfe++nNde/iTrYgn+tXcBBzfOY+WmLvzeXpylS+henaBvuXLJkr28LLWNBitFd5DlmcJCnupeSK4jTe2RMEbf7s6G9vyK4ilWIh7G6Kfi+GkXL2VRSspQfL6fUIJ4gBX3icXC+Py0WyRlF0nbhaFi6IkoPj9hlcLtYxRPKQutjaZaoTVDlSizNjKnGib6ZJSlFzYQlj0c3QwGg+G0YzKjd0TkOhHZKiI7ROQzYxyPi8jt0fE/isjSimOfjfq3ishrJuPejrnSV9VfRJtZVf3RqIG+bTIGYDAYDDOOSTLvRPVE/pmwiNR+4AkRuWtUgfMPAN2qulJE3gn8LfAOEVkDvBM4lzBU/gEROUtV/ZMZU7W/AT9bZZ/BYDAYhrkE2KGqu1S1CPwQeNOoc94E3BZt3wm8SkQk6v+hqhZUdTewg+OsRTIWE9n0rwdeCywQkX+sOFQLeCd7cYPBYJiJHIfPqVlENlTs36Kqt1TsLwD2VezvBy4d9RpD56iqJyK9QFPU/9io5y6oemTjMFH0zgFCe/4bGWnD7wc+ebIXr5ZSc4q7z7+NP93zBjb/4mwWbHiUnj+5nD992cPMs9N8dPd1zP29hb6wE6e1hUPrMjSde5gb6jdynuuRU+F3uRU8emAZqb0ONftK2B3d+P39oXBWJo021ODVJ8k12hTroVTnk6rNMzfdz7x4L3OdXuqtHDVWQBwXV+wRiS4l9SlpQEEDSgj5clJWuVU4cfOBSymwKAbOCJE1L7Dw1RoSVgsY5cCFEYlZRydUydihZJVCa9UmZlUw0xKzAN607bXM/dYmjvzJOn7ykr/jq93nseDHLpnfvoCsWsGha4v8deuDvOjBv2x9OXM2BOjOF7FSKXJnt9JztpJY2sd1Tc+y0gkd8TtKLo/3r2BXRzPxQzapDiV+JI/0DRAM5giixCzLdZB4HFJJgnQcL2VTSlt4SaKmBHGFhI8b90jEPFJusSIxq0TcClvZietGlbNi4hPDHzcxy6acjCXHJb4WPsc4d6um+kCGI6q6fiqHMtlMZNN/GnhaRL6nqmZlbzAYTn8UCCbt1dqARRX7C6O+sc7ZLyIOUEeY/FrNc4+bY371i8gd0eZTIvJMRdssIs+c7MUNBoNhJjKJ0TtPAKtEZJmIxAgds3eNOucu4KZo+0bgN6qqUf87o+ieZcAq4PGTvbeJzDsfjx6nTTvfYDAYTjmTZMKMbPQfBe4FbOBbqvqciHwR2KCqdwHfBL4jIjuALsIvBqLz7gCeJ/ShfuRkI3dgYvNOe7R5BMipaiAiZwGrgV+d7MWrpaZ5kC4/wdbvnc2Su/fBimV03TDIXzQ9zU8Hm9ny2xWseLQN3/fJn7+ErnU+H1+ygQtjSlwSbCwWuffIefTvqaPlRSXZ1o929aC+j5VMIrW1FJtSFOpd8o1CoUGx6ou01gywMNnDvFgPLVFiVkrCpKzKxKySegQElAgoqkb2fGc4OWuUPd+LkrJKlUJrkV3fD0Kbvh+ECVrHSswqF07R0fb9iYTWxhNfY4zjU8BkqBP2//0i0s02Z/3pFubYNv/0q+tYed/TBMUSB997Dp+85G4WOzX89aEL8R+rp/apA3jZLPa5Z9G5xqX+rE6umLublyb3kLEytPn9PJE7hyc7F+IdTFLXAcnDJeyuUGgtKBZBg1BoLR6HZJiY5aVcSmmbUkrwkuAnh4XWrHgotJZ0Q6G1lFMk7RRIWUVSVpGEVIit4eOKhythclZlYpbFyMSsyiSsahOzjD3/OJlEv5Wq3gPcM6rv8xXbeWDMEHhV/RvgbyZvNNWHbP4WSIjIAuA+4L3ArZM5EIPBYJgJVGvaOd2llUVVs8BbgP+rqm8jTBgwGAyG0w9TRAURkcuBdwN3R332Mc43GAyGWYtZ6cMnCDNwfxo5F5YDD03ZqEaxyM3y2kc/wtwfbsF7sY0X3zqPr130A7Lq8fln3siCh4t4e/ZiL1vCofUxLlq7i9fXPEtcXDr8AX7Vdz5PvriIzG6Lmr155FAn/mAWsW2s2hqCphryTTFyTTaFBvDqPerrsizI9DI/0c0Ct5smK0uNBYnInl+2kQYEBCh59clrQEGlQmgtFFkrVNjzC4FDwXcioTU7FFobFaPvR9uqDNn1AxU0kLFj9BWGYvTHQCpj90cUUK9gmmL0J+sfSvyXj7P9I4u4dckDvHfnDaz8fh9iWfhXrCV9/SE+WLed3+XhzicvYt5jefy9+3Bamuk9r5G+c0tcv/B5Xt/wFIvtBDktsLnYxO97VtLW3kCy3SJ90Cd2JAu9fQSDWdAwhk9iMSSVhEwKPxPHy9iU0oKXAj8Vxegng+EYfbdEJlYg4xaG7fl2Ycien5ASCSlGhVNCO36MAFcUV0bF5TNs1z/eGH3DcWKklfUR4BERyYhIRlV3AdOjsGkwGAzTySxexVdDtYXR14rIU8BzwPMislFEjE3fYDCcnpzGK/1qzTtfBz6lqktUdTHwF8C/Td2wDAaD4RRyGk/6VZl3gLSqDtnwVfVhEUlP0ZgMBoPhlHI6m3eqnfR3ich/Bb4T7b8H2DU1Qzqag36cs79uQbGEd81LeMmbn+PapMcn219O4oFa4n/cDDU1dF06h2B9PzfP/Q9WOBn2eP1sKszlVwfWYO9IUrfHJ9bWQ9DTCxpgZerQxnryLUmyLRbFWig2BCQa8szL9LEo2c1Ct4sWu48ayyMlDg4jhdZ8DYbE1kpKmJgVhI7cwSBONohTipK1CsFw8yoSs4qBHVXKilqUkBUEFkEQOnHLQmsaWMOJWRUJWsOrj5GJWWN+eI8ltFaRmDVTnbgAxesv5itvvZWH8wnavrucpicfJffmSznwCuEHZ99OSQP+26430PI7B/eZHQRiUVq9iM7zLNadtZvX1W1ilVvAlTTbSkV+338WTx+cj9sWJ31QSXYUsDr7CAYGUa8UCvPZNpJMQDqFn0lQqnEpZqzQkRuJrQWJACvhYcd8kvES6ViRjFsk7RTJ2AUyTp6ElEhZhciBGyZkuUOPwVFCa6OdtuV9k5g1hZzGk/7xFEZvAX4C/BhojvoMBoPh9OI0T86aSE8/AfwZsBLYDPyFqpamY2AGg8FwypilE3o1TGTeuQ0oAb8DrgfOIYzZNxgMhtOXM3jSX6OqawFE5JtMgqznidB5pBbr+SfpvOlyjryqwE8X3cv9uRp++ZuLOeuBQ3gDgwQvX0fHpcoHz36cVyR6GAjg/sGz+G3PWbTvaKZll5Le3Y92HCEoFrHiCaS+lmJLmlyrQ74JSrUKjUXm1PWzKN3N4ngnc51emqw8NZZFXBxcGf6TBQR4+BTwyasyqDaD6pLVGINBfITQWmEoSaucmDUqOUsr7fnHFlor2++HhdbkaHt+JWMJrY3Fsez5M4zEXx/gmmQ/5/z0I6y+4zk45ywOvLXIzec/xsVxly8eOZ/23y5k2e8P4XV145y9kkMXJomv7eHGORu4IOaTlDQd/gCPZlfxu8MryO2vobYd0u0e7uEBtLefIJcHosIpsRiSSqGZJH7GpZSxKWYsvBRhclYygKSPm/BwXZ90rEhNrEDGKZC2C6Ts4rDYmlVOzvKwCHAlbOMJrZUTs47Xnm84foSZ//k/GSb6lAyZco63iIqILBKRh0TkeRF5TkQ+HvU3isj9IrI9emw4gXEbDAbD1KAgQXVtNjLRpH+BiPRFrR84v7wtIn0TPNcj9AGsAS4DPhJVd/8M8KCqrgIejPYNBoNh5nCmxumr6gmLqkVa/O3Rdr+IbCEs6vsm4KrotNuAh4FPn+h1DAaDYdKZpRN6NVQbp39SiMhS4ELgj8CciuIsB4E54zznQ8CHABKSRi49n8x72/j75b8grz4f2/gult5dwNu+E2f1KvZcmeCy9Vt4d91GMlYND+Zs7mx/CTsPtFC73aZuZw6r/TDewGAotFZfi99ST641Tq5ZKDQrfo1Hc+MASzLdLE8eYYHbTas9QI0FKXFGFE4JCOPzxxNaywZxskGMbBCjFDhk/dhRQmtle76vFp5vUyoXUakooFIWWgsCGRJa08jWf0yhtXKM/nhCa0c9TmTvP/bhiZhsG+ndZ/+S12x5M6u/3gtBwO53NvPVy77NNcl+7s8lufUPV7LyoRzejt04rS10X9RC77oiNy/fxKtSL5KUDDkt8EShmd90ncOL+5tJ77eoafOJHxyE7t4hoTWxbSSVQuIxyKTwauMUax2KNYKXJmypUGjNSfgk4iUSrjdCaK3WyZOx86StAmkrElyTUhSfHxDDP0pobbQ9/1iY+PzJ5Uy26Z80IpIhjO3/hKqOMAlFdSDH/POq6i2qul5V18es5FQP02AwGIY5jc07Uzrpi4hLOOF/T1V/EnUfEpF50fF5QMdUjsFgMBiOi2ly5FYT1CIi60TkD1EwzDMi8o6KY7eKyG4R2RS1ddVcd8omfRERwoK/W1T1/1Qcqqz8fhPw86kag8FgMJwQ07PSryaoJQu8T1XPBa4D/kFE6iuO/5WqrovapmouOpUr/SsIa+leXfFN9FrgS8C1IrIduCbaNxgMhhnDNMkwvIkwmIXo8YbRJ6jqNlXdHm0fILSMtJzMRafMkauq/wHjep9edVwvZgnbPhzjidU/pMFK8tYdb6HxZ2mcRzditbZw6BUt1FxxmI/Ne4DFTg3PlbJ89/Br2P7CApJtNg1bS7h7D+N3doUvV1eLtjaSn5dkcK5FrhW0qUA6U2BxbQ9LU50siR1hrt1Lo+2REZe4uCOqZZXUp6AeBQ3IqzCoDtkgTMoanZhVrpiVC9woKcuh6IdO3FJgU/Jt/Mh5G0RO2tFCa6igAUNJWiOTsji20NpETtxRzGShtTJf615O4WvzsTb/ka4PvJQb3/Q7XpfK86KX5y83v48F91nYG7dCMkHuwqV0rIdXrNnGW+s2MtfO0OEPsNeLc1/PWp7cv5D43hiZNiV1IId1pIegt39IaM1KJpF0CpIJ/LpkKLRWa1PMCKU0lNJKkAqwkh7xRJF0vEjKLVLrFqiJnLgpO6yaFa+omOXiDzlxxxJag+HKWVBO1Bpep03k3DWcBNV/ZptFZEPF/i2qekuVz60qqKWMiFwCxICdFd1/IyKfJ/qloKqFiS46LdE7BoPBMGs4PtPNEVVdP95BEXkAmDvGoc+NuKSqioy/PIr8n98BblLVsjfhs4RfFjHgFsLQ9y9ONGAz6RsMBkMFwvgmiuNFVa8Z9zoih0Rknqq2HyuoRURqgbuBz6nqYxWvXf6VUBCRbwN/Wc2YTCCvwWAwjGKaZBgmDGoRkRjwU+DfVfXOUcfKUZBC6A94tpqLzoqVfqElzu2v/FcGg4B/7FzL7p+sYP6vt6COQ+8rVtB7VZYvrbqXS+IWbX4/3+u+gke2rqLueZvMAZ/Uzi6CQ0dQ38euqUFamsjNyzAwzyHXCl5rkZamfprTg6zIHGFl4hDznW5a7Dw14hAX5yh7/nBilpINHAaDGIMjkrLiZP04WT9GSW1yUXJW0Q/t+cUhW35ZaK0yKYsRSVmjC6dUbc+P9oe2x3qk4hwm3/4+VUkut379elp/9ijF6y9m7nt38/mWTfwiW8s32l6De089tb/dhl8ooBefR/vlLqsu3Muftv6Oc9wEA0GePxZaeSq7hEfaVqC709S8qGT25bEP9RJ09xAUIqG1RBypyaC1GTQVo1gXo1hnU6wRShkoZcBPBZDyiCdLZBJFauN5Uk6JuliOWjdPjZ2nxqpMzCqGyVmWN2TPd0VxxxBaA0YIrZUxhVOmmOmJwf8ScIeIfADYC7wdQETWA3+mqh+M+l4ONInIzdHzbo4idb4nIi2EP0w2EcrgT8ismPQNBoNhWpmGSV9VOxkjqEVVNwAfjLa/C3x3nOdffSLXNZO+wWAwVDKLq2JVg5n0DQaDYTRm0j+1zG/s4hy3iVc9/X76nmhh+Z178fsGKL5qHW2v9vmrCx7k9akueoMSt/edz0+2XUBmc5ym5wrEDg8QtLUTFPJhvHVLE4X5dQwscMnOheIcj7rmAZbXd9Ea72dV4iCL3E7mOlnqLJukuEOFUyrj80sEYxZO6Q8S9PtJ+v0E2SBGzo9RUmvMwinlGH0vsI6K0x9dOEWDYbG1Y9nzhziWPb+Siez5J/Hhn8rV0tyvP4lespauDw/y85U/5ZmSzSf+8A7STydYdO8+vMNHsNeuZv+VaeouOcyHFz7CJfESJYUNxSQ/OXIRW7tbGdhVT90eqHmxROxAL9rZNVw4JZ7AqslAbQa/PomfcijWORRqLYo14GXATweQ8YmlStQk89QlctTH8iTsEjVOnlo7R8YO7fopqzBUOCUm/pj2fHcMoTVTCH36ma1a+dUwKyZ9g8FgmE6MecdgMBjOFGaxgmY1mEnfYDAYRmMmfYPBYDgzON0Lo8+KSb/B8njb9rfg/HsTy588hNfWTvDydex9nc2HLnuI99XuoKRwZ/8Kvr3tcpynMjRvLhHffhDt7SPIZkOnXGszxYUNDCyKMThfyM/1SbcOsrKxk3Nq2ml2+lke62CB3U+9JaQqnLjAkBM3qz5FVfrVpj+I0R8k6AuS9PtJBisqZuV8l1zgUgps8r5L3nfIR8lZpSBKzFLBDwTPt4dE1nREYtaw01b1GE7cCBn903TMZKzy4zGcuDP8Qy/LF7P9zx0euehfOBwI79v4QRb82KXmuUN4e/fhnL2SA69sRK7s4WMrfsNrUl1Y2Gwswp1dF/P7PcvwO5LU7hTqdpdI7OtBD3fiDwyCBlixGFZtBmpr8OtTFOtjeCmbfL1FsRZKNVCqCdCMRyxVJJMsUJvIUx/L0xAbJG551Dk56pwsNVboxE1bBdLlxKyoYpYrim2cuDOPGf75PxlmxaRvMBgM04aCBKfvrG8mfYPBYBiFMe8YDAbDmYSZ9E8tuwp1ON9YTP3PnsIr5NEr17H7zTHe84r/4GMNz2CLcHv/Iv7v9pcTbKyj9akSyS3t+Ac7UK8U2vPnNFNa3Ez/kgQDC4XcfJ/U3AHOaj7Mmpp2zkkcoN4eZIHTR6MtZCRGXNyhMRS0NGTPH1QoqUU2cEfY8/uDBAN+gn4/Qc53GfTj5HwXL7BDW35gD9nzPd+mFFh4vkUQJWWVRdbKRVQqE7JCsbVj2PPLQmuV9vpTaM+f6pXSC5+u5Tcv/wd8hbc+9UEav58hfd8zeNkszsrltF/TSvEVffzV6gd5U/oAcYmxseDzg86Xcv/O1dhb06SPQP2OEsm9PeihIwQDA8P2/JoaqKvFb8pQaIyTb3LwEkKxltCmXxugNR5uukQmVaAukaMxnqMhlqXezZGwStTZOWqsPDV2blx7vknKmpmYlb7BYDCcSZhJ32AwGM4QjOCawWAwnDkIRnvHYDAYziz09F3qz4pJv3Q4Tv2PNmI11OOdfQ673hrn/a98hE80bsIW4Qf9i/nqC1fjP17PnI0lks8dwG8/iAYaOnHntlBcEjpx+xcLuQU+6XkDrG7pYG1tG+ckDrA8dphaKVblxO0PHPLqVOXEzfsuXmCNSMqqdOKOSMoqJ2TpFDhxq62UNQucuAC/e9U/UFJ47YYP0/idDOlfPw0iOKtWcOC6OXhX9/KZNfdxY6aNuMR4vBDw3c4ruG/HauwX0jRsUxKdHsk93eihI/i9fSOduA11I5y4+QbBj0OxbtiJG8sUyaQKNCSzNMZzNMUHqHdzNDqDxK1SVU7cmMiISlnGiTszMOYdg8FgOFM4zQXXzPLAYDAYRjEdhdFFpFFE7heR7dFjwzjn+SKyKWp3VfQvE5E/isgOEbk9KqI+IWbSNxgMhlFMx6QPfAZ4UFVXAQ9G+2ORU9V1UXtjRf/fAl9R1ZVAN/CBai46K8w7Vtcg1tI57H/LIvrWlPirK+7mA3W7yWrALT3ncstzVxJ7PMPcjQXiL7ThHewAwGluhJoMhSVN9C2NMbBIyC/wqJvXx+qmw5xfu59zEm0sdTuZa3vExSIjsTErZWXVpz+APo3RH8QpqU1fkKTXT42olDXoxckF7pA9P+85eGpR9G2KvjOiUtaY9vzAOu5KWTJq/1TZ86fTDnrYd3nr7/4zS75rE3vwSSSTZuCVZ9OzwiZ19WG+sOpXXJ/qASx+m7f5zuGX8fC2s0i8kKBhW0DNrgGs3ix66PCwyFo8gVUX2vO9xjSFpjj5RptCvVCohyAW2fNrPWLpInWpHPXJclLWIE3uYCiyZmdJWKVxRdYSEmCBsefPVJTpcuS+Cbgq2r4NeBj4dDVPFBEBrgb+pOL5/w34l4meaz4tBoPBMArR6tpJMkdV26Ptg8Cccc5LiMgGEXlMRG6I+pqAHlX1ov39wIJqLjorVvoGg8EwrVQ/oTeLyIaK/VtU9Zbyjog8AMwd43mfG3E5VRUZ92tkiaq2ichy4DcishnorXqEozCTvsFgMFRwnEVUjqjq+vEOquo1415H5JCIzFPVdhGZB3SM8xpt0eMuEXkYuBD4MVAvIk602l8ItFUz4Fkx6Usywc4PLuLq657kdQ1P87pUnhe9PP/U+XLu3PQS6p+I0fLkIM72/XidnYjjYrc24y1upVgfo3+Jy8BCKC4s0jqnl3Ob2jk3c4Cz4+0sdbtosQLqrDgWMsKeX9ASefUje75FfxCjL0jQHyQoqUOPnxqKz8/6MQYqYvPLRVOKflgspRTYQ/Z8z7fCAiq+NcKejwoaRMVSqrDnS6XNfiJ7foUtH2a3PR/gxp99nFXfHUQ3PIm9YD5Hrl1C5zV5zl3UzmcX380lcYuBwOP+XCvfbb+cTVsXU7vFpX6HR3pXL9J+hCCXI8hmQSysVAqrrhZtrMVrTJNvipFvsCjUC8V6KNUqQSyAjEcyU6AulY/i87M0ulnq3Sx1TpZ6Oyya4opH2iqQkiIJyyMhHjGi+HwBG8GNbPnAhPb8sWz5lecZJhHV6bLp3wXcBHwpevz56BOiiJ6sqhZEpBm4Avhy9MvgIeBG4IfjPX8szCfGYDAYRjFN0TtfAq4Vke3ANdE+IrJeRL4RnXMOsEFEngYeAr6kqs9Hxz4NfEpEdhDa+L9ZzUVnxUrfYDAYppPp+OWqqp3Aq8bo3wB8MNp+FFg7zvN3AZcc73XNpG8wGAyVKGDKJRoMBsMZxOk750/dpC8i3wJeD3So6nlRXyNwO7AU2AO8XVW7J3qt/ByLv7jxZ9xcuw9XHH6fD/jyvht5bsMy5m5Q6p85gu7ej5fNhg65eXMoLG2kb0mMQq0wuEiR+TlWtHZyXn07a9P7WR7rYJHTT6NlkZHEkAMXoKQeBfUo4NMfBAyqTX8Qoz9I0OOn6Q8SFNUZElgrO3AHvTh536UYVcoqeA6lwCZQwfNDZ67vh4lZQWCNSsqqEFkbEkwTCDh2QhaMdOJOUCVr6LmMcd4JcKqEqc7++314+9uwLljDi9fVU/vKQ3x15T2sjR1msVPDi14/d/afzx17L+LIC800bRXqdxSI7+lCO47gRQlZYttYmQxSV0PQVEupMUm+0SXfaFGoCwXWvJowIctyfdLpAnWpHE2JLI3xQerdHA3OsBM3ZRWotXJYKGmrMCIh61hVsqCcpGUSsmYCp7Pg2lR+cm4FrhvVV23ascFgMJw6yhE8E7VZyJRN+qr6W6BrVPebCNOFiR5vmKrrGwwGwwmh0xa9c0qYbpt+tWnHiMiHgA8B2E31Uz8yg8FgoJycNTtX8dVwyhy5E6QdE6Uy3wJw3vkx/U917TxWgEezy4YE1pY+VSD+fIXAWkszwcJW+pfW0LfEZnCh4teML7BWZ7nExR2yj44tsBanP4jTHyTp8xP0B0kG/AT5wB1XYC3vO8PJWL6FrxMIrEUJWWWb/oQJWWPZ8sd6rDyH2Z+QVYkODJJ786XsvxbefNkf+VjzIyx1aihogofzwncOXzcksDZ3W0DNrn6s/R0E3T0ExSJi24hzDIG1OijVKX6Nj5UpkUkXSLilcQXWaux8lIwViqzZ6LgCa65YR9nyoTqBNWPLnyZm6Sq+Gqb7E3QoSjfmWGnHBoPBcCoR1arabGS6J/1y2jEcR9qwwWAwTBt6HG0WMpUhmz8g1IpuFpH9wBcI04zvEJEPAHuBt0/V9Q0Gg+HEUMQkZx0/qvqucQ4dlXY8EbbAf+04j+8/czF2W4LWjQF1Tx8ieLENr5DHzmSQeXPIL22gd2mMwYVQWFiiaW4vzalB1ta3sybZxqrYQRY4gzRaDsmK2PyyLb+kPln1yKrSHzj0BXH6NTFU+LxcLGXAD4uoDHqVAmsOxcAeis2vFFdTFfxA8APrqOLn1RZLOS5xNTi2LX/0ucfJTIhh3vPna1jwin18a9ndvDzhkw1cfpFN8XR2MT/ecwGDWxpo3CbU78gT29uJHunEGxgAwEomsWprIJkkaMxQbEqGxc/rLQr1UKqFUq0PGY9EpkhtJK6WckrUx7I0xbI0OINDtvwaKzdCXC2GP6L4+Xix+aZYygxmlppuqsFk5BoMBkMlOnvDMavBTPoGg8EwGrPSNxgMhjOI03fON5O+wWAwjGa2hmNWw6yY9LcOtOB/92Use6pA7FAnumtfKK6WTOIsX0pxcSO9y+IMLBLyC8JkrHVRMlaz08+q+EEW2P002xYZSR7lwC0nYxVV6a8QV+sLkvT6Kfr9JNkgRr+fGErGKgX2kAO3XCGr6DtDDtzKZCzV0QlZo8XVxnfgQoW42ngO3BF9p7cDt8z/eu+/c32qB4Df5mN85/CreHjbWciROHVbhUU7iiT2dKGHDg+Jq1nxkclYfsoh3zAsrlaqg1JtKK4WSxepSeVpTGaHkrGSdukocbW0VSAtRRJSOioZyzhwZykK+DPowz7JzIpJ32AwGKYLYfYmXlWDmfQNBoNhNKfxpG9+NxoMBsNopkFaWUQaReR+EdkePTaMcc4rRWRTRcuLyA3RsVtFZHfFsXXVXHdWrPRjHbDotm14h4/gA3Ymg7NqxZjJWBc3HmJtTRtnJ9pZ6nRSY3njJmNltURWfbKB0Kcx8oEzbjJWzo8x4MciO76LF1jjJmP5gUVQkYylyqlNxprFwmrjcU2yk/tyjXz34OU8sW0p6S0xFmz3iXcXxk3G0sZ6Sk0pCo0x8o02XkKOmYzVGM/SEMtGwmpZElIaNxkrIX6YjCVEtnuMLX+2okyX4Fq5vsiXROQz0f6nRwxF9SFgHQwVodoB3Fdxyl+p6p3Hc1HzqTIYDIZRTJPg2vHWF7kR+JWqZk/mombSNxgMhhEoBEF17eSour5IxDuBH4zq+xsReUZEviIi8WouOivMOwaDwTBtKMdjr28WkQ0V+7dEtUAAEJEHgLljPO9zIy45QX2RSIp+LXBvRfdnCb8sYoS1Rz4NfHGiAc+OSX8gh1oFnLNX4ten6F2aon+RxeCiAHfeACtaj7C27gDnptpY7nZUiKq52BIfso2WC56PFFVLD9nx8+rS7yfoDxIMeInhIikVomr5ciy+Wni+TSkIbfjlgucjRNUCa8hOr1G8/pCNfiI7PhXnjeir+LuchgXPq+XKjTeT3dJA/TZlxY48sb37CA53osUSnldCHBe7vh6pryNoqiHXlKDQ6JBvGC54HsTDIjtOpkRdOk9DMkdDIkfTUMHzkaJqrvhRsfMwJn/Ijm8Knp9+VL+IP6Kq68c7qKrXjHdMRA6JyDxVba+ivsjbgZ+qaqnitcu/Egoi8m3gL6sZsPmEGQwGwyimyaZ/PPVF3sUo005FQSoh9Ac8W81FzaRvMBgMo5mGkE3C+iLXish24JpoHxFZLyLfKJ8kIkuBRcAjo57/PRHZDGwGmoH/Wc1FZ4d5x2AwGKYLVfCnPmZTVTsZo76Iqm4APlixvwdYMMZ5V5/Idc2kbzAYDKM5jTNyZ8Wkr7UpDr1nLQMLQ0GsxLxBVrUc5vy6NlYnD7Ai1sF8O0d95Lx1pQYIk7B8Dchqkax69CtkA5u+IEVPkKI/SA47bv0EpcAeSsQajBKxir495Lwt+E6YfKWh0zaIkrFGiqmFzluThDW1zPnbOM6Lewk6uwhyOTyxsJIJrNoM0lCP35Ah35QIK2I1CMU6KNaCV+sjmRKJdJF4zAuTsBI5mmKD1LtZGtxB6uzcUAJWZRKWTTDCeRuTchKWZZKwTjfMpG8wGAxnCAqYGrkGg8FwpqCgp2+9RDPpGwwGw2iMeefUonN8Lnr/05ybPkCz0zeGDd8BaggIKGiJgaBIVn36A4uSWvQFcXqC+qNs+AN+gqzvkvNdsl4cT61xbfhlITU/suGHmdoT2/DDfUbY8KVif+jYWI9gbPjj8YdnCJIJrIZ6ZMViSo0pck0xvKQ1rg2/uUJIrdHNkrSLx7ThJ8QjRjCOkJo1woZfaZc3NvxZjjIt0Tunilkx6RsMBsO0Ylb6BoPBcKYwKYlXMxYz6RsMBkMlymQoaM5YZsWkvyrZyb8u/I8KW6hDQRMU1KMzyDOo0B84UTHzOvqDJD1+Koy9V5t+P0HOdxn0K8TTPCcsglIWUPMtAgTPt4YKmh8Vfx8MF0MBjlnQvGwvH47Dr148zdjuJ6bv3ZdW2O4Vv9bHyhRxYj416TzzEjka4zma4gPUuzkanUEydp56OxvZ7QvExB9R0Pzo+HvBxQYw8fdnGmalbzAYDGcQZtI3GAyGMwRV1PdP9SimDDPpGwwGw2hMRq7BYDCcQRjzzqklqxZ3ZzP0+QkK6tLvh0lWQ5WtApdBb5RAWmBT8m38wMILQoG0iRy0VQukVTpnGd4/qvJV5TkYB+1kctEnnqTByVLn5EZUt4qJF1W1GlndyoIhB60rYWIVlB20FqHb1ThoDYQTvoneMRgMhjMIs9I3GAyGMwXjyDUYDIYzByOtfOrZ09PCJ3/53im9hox6NMxs/mn+4xOc4UbNYDgBTmNp5VPikRKR60Rkq4jsEJHPnIoxGAwGw1gooIFW1U4GEXmbiDwnIoGIrD/GeWPOlyKyTET+GPXfLiKxaq477ZO+iNjAPwPXA2uAd4nImukeh8FgMIyJRkVUqmknx7PAW4DfjnfCBPPl3wJfUdWVQDfwgWoueipW+pcAO1R1l6oWgR8CbzoF4zAYDIYxmY6VvqpuUdWtE5w25nwpIgJcDdwZnXcbcEM11z0VNv0FwL6K/f3ApaNPEpEPAR+Kdgu7P/6Xz07D2KaLZuDIqR7EJDLt92N/fMovYd6jmc2x7mfJybxwP933PhDc0Vzl6QkR2VCxf4uq3nIy1x/FePNlE9Cjql5F/4JqXnDGOnKjP9wtACKyQVXHtXnNNsz9zHxOt3sy91M9qnrdZL2WiDwAzB3j0OdU9eeTdZ3j4VRM+m3Aoor9hVGfwWAwnFao6jUn+RLjzZedQL2IONFqv+p59FTY9J8AVkWe5xjwTuCuUzAOg8FgmOmMOV+qqgIPATdG590EVPXLYdon/ehb6aPAvcAW4A5VfW6Cp02mjWwmYO5n5nO63ZO5nxmGiLxZRPYDlwN3i8i9Uf98EbkHJpwvPw18SkR2ENr4v1nVdfU01pgwGAwGw0iMXKDBYDCcQZhJ32AwGM4gZvSkP1vlGkTkWyLSISLPVvQ1isj9IrI9emyI+kVE/jG6x2dE5CWnbuRjIyKLROQhEXk+Shv/eNQ/K+9JRBIi8riIPB3dz3+P+sdMaxeReLS/Izq+9JTewDiIiC0iT4nIL6P92X4/e0Rks4hsKsfCz9bP3Exixk76s1yu4VZgdKzvZ4AHVXUV8GC0D+H9rYrah4B/maYxHg8e8Bequga4DPhI9F7M1nsqAFer6gXAOuA6EbmM8dPaPwB0R/1fic6biXyc0NlXZrbfD8ArVXVdRUz+bP3MzRxUdUY2Qo/2vRX7nwU+e6rHdRzjXwo8W7G/FZgXbc8DtkbbXwfeNdZ5M7URhoZdezrcE5ACniTMcjwCOFH/0OePMHLi8mjbic6TUz32UfexkHASvBr4JaFg7Ky9n2hse4DmUX2z/jN3qtuMXekzdvpxVWnGM5Q5qtoebR8E5kTbs+o+I1PAhcAfmcX3FJlCNgEdwP3ATsZPax+6n+h4L2GI3EziH4C/BsoqYMdK058N9wOh4OV9IrIxkmWBWfyZmynMWBmG0xlVVZHZV+FWRDLAj4FPqGpfqPkUMtvuSVV9YJ2I1AM/BVaf2hGdOCLyeqBDVTeKyFWneDiTyZWq2iYircD9IvJC5cHZ9pmbKczklf7pJtdwSETmAUSPHVH/rLhPEXEJJ/zvqepPou5ZfU8AqtpDmNl4OVFae3SocsxD9xMdryNMg58pXAG8UUT2EKowXg18ldl7PwCoalv02EH4xXwJp8Fn7lQzkyf9002u4S7CVGkYmTJ9F/C+KPrgMqC34ufrjEDCJf03gS2q+n8qDs3KexKRlmiFj4gkCf0TWxg/rb3yPm8EfqOR4XgmoKqfVdWFqrqU8N/Jb1T13czS+wEQkbSI1JS3gVcT6s/Pys/cjOJUOxWO1YDXAtsI7a2fO9XjOY5x/wBoB0qEtsUPENpMHwS2Aw8AjdG5QhiltBPYDKw/1eMf436uJLSvPgNsitprZ+s9AecDT0X38yzw+ah/OfA4sAP4ERCP+hPR/o7o+PJTfQ/HuLergF/O9vuJxv501J4r//ufrZ+5mdSMDIPBYDCcQcxk847BYDAYJhkz6RsMBsMZhJn0DQaD4QzCTPoGg8FwBmEmfYPBYDiDMJO+wWAwnEGYSd9wShCR/yYifzkTrjNdYzEYZgJm0jcYDIYzCDPpG6YNEfmciGwTkf8Azj7GeQ+LyFdEZIOIbBGRi0XkJ1HhjP9Zcd6nROTZqH1iouuIyAoR+XWk2vg7EZm1ImsGw4liVDYN04KIXESoC7OO8HP3JLDxGE8pqup6Cat0/Ry4COgCdorIVwjrFbyfUAdfgD+KyCOEC5nxrnML8Gequl1ELgX+L6E4mcFwxmAmfcN08TLgp6qaBRCRicTzysc3A89pJJ4lIrsI1RSvjF5vMOr/SXQNa6zrRLLQLwV+VCEJHZ+cWzMYZg9m0jfMVArRY1CxXd4/kc+tRVhUZN1JjstgmNUYm75huvgtcIOIJCPJ3Dec5Ov9Lnq9VCS9++aob8zrqGofsFtE3gZDhbQvOMkxGAyzDrPSN0wLqvqkiNxOKJXbQVgv4WRf71ZCaWCAb6jqUwDHuM67gX8Rkf8XcAkLjjx9MuMwGGYbRlrZYDAYziCMecdgMBjOIIx5x3DKEJF/JqzvWslXVfXbp2I8BsOZgDHvGAwGwxmEMe8YDAbDGYSZ9A0Gg+EMwkz6BoPBcAZhJn2DwWA4g/j/AY9iipfX2yCuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(pos_encoding[0])\n",
    "plt.xlabel('d_model')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder 本人\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model \n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "    \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "        ## num of layer\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        input_seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) ## scaling\n",
    "        x += self.pos_encoding[:, :input_seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        ## go thorough those layer\n",
    "        for i, enc_layer in enumerate(self.enc_layers):\n",
    "            x = enc_layer(x, training, mask)\n",
    "\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    " \n",
    "    def call(self, x, enc_output, training, combined_mask, inp_padding_mask):\n",
    "        tar_seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}  ## storing each encoder weights for analyze\n",
    "    \n",
    "    ## same as encoder\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :tar_seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i, dec_layer in enumerate(self.dec_layers):\n",
    "              x, block1, block2 = dec_layer(x, enc_output, training,combined_mask, inp_padding_mask)\n",
    "    \n",
    "              attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
    "              attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
    "\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally! we made it\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask, combined_mask, dec_padding_mask):\n",
    "        \n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, combined_mask, dec_padding_mask)\n",
    "    \n",
    "        final_output = self.final_layer(dec_output) \n",
    "    \n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try with our easy examples\n",
    "\n",
    "num_layers = 1\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "dff = 8\n",
    "\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "output_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "## train to predict next word! as we say in seq2seq train\n",
    "tar_inp = tar[:, :-1]\n",
    "tar_real = tar[:, 1:]\n",
    "\n",
    "inp_padding_mask = create_padding_mask(inp)\n",
    "tar_padding_mask = create_padding_mask(tar_inp)\n",
    "look_ahead_mask = create_look_ahead_mask(tar_inp.shape[1])\n",
    "## combine them jiust choosing value 1 in tensor\n",
    "combined_mask = tf.math.maximum(tar_padding_mask, look_ahead_mask)\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, output_vocab_size)\n",
    "\n",
    "predictions, attn_weights = transformer(inp, tar_inp, False, inp_padding_mask, combined_mask, inp_padding_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4205   65    6    7  223 2078 4206    0    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3 4206]], shape=(2, 10), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[4205   65    6    7  223 2078 4206    0    0]\n",
      " [4205  165  489  398  191   14    7  560    3]], shape=(2, 9), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[  65    6    7  223 2078 4206    0    0    0]\n",
      " [ 165  489  398  191   14    7  560    3 4206]], shape=(2, 9), dtype=int64)\n",
      "\n",
      "(2, 9, 4207)\n"
     ]
    }
   ],
   "source": [
    "print(tar)\n",
    "print(tar_inp)\n",
    "print(tar_real)\n",
    "print()\n",
    "print(predictions.shape) ## batch, length, prob of next single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "## what's our loss function???\n",
    "## just like classication problem\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "## but our loss should not include\n",
    "def loss_function(real, pred):\n",
    "    ## inverse our mask\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    ## multiply it! we done!\n",
    "    loss_ *= mask \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lr-equation.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our opt\n",
    "## inside warm up steps  up up up\n",
    "## outside smoothing decayed\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  # 論文預設 `warmup_steps` = 4000\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4 ## 先用 4 層,我電腦會爆掉\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 512\n",
    "\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "dropout_rate = 0.1\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,input_vocab_size, target_vocab_size, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "### all the mask create\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trans/logs/4layers_128d_8heads_512dff_20train_perc/4layers_128d_8heads_512dff'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train form scratch\n"
     ]
    }
   ],
   "source": [
    "## save cktpts\n",
    "train_perc = 20\n",
    "\n",
    "run_id = f\"{num_layers}layers_{d_model}d_{num_heads}heads_{dff}dff\"\n",
    "checkpoint_path = os.path.join(checkpoint_path, run_id)\n",
    "log_dir = os.path.join(log_dir, run_id)\n",
    "\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,optimizer=optimizer)\n",
    "\n",
    "## only save the 5 closest ckpt!\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint: ## if ckpt exists, dont train\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "    print(f'latest checkpoin in {last_epoch} epochs')\n",
    "else:\n",
    "    last_epoch = 0\n",
    "    print(\"train form scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### path is wrong\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint: ## if ckpt exists, dont train\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "    print(f'latest checkpoin in {last_epoch} epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function \n",
    "def train_step(inp, tar):\n",
    "\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "  \n",
    "    ## creating masks\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    " \n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already train 0 epochs。\n",
      "last epochs：-40\n",
      "Saving checkpoint for epoch 1 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-1\n",
      "Saving checkpoint for epoch 2 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-2\n",
      "Saving checkpoint for epoch 3 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-3\n",
      "Saving checkpoint for epoch 4 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-4\n",
      "Saving checkpoint for epoch 5 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-5\n",
      "Saving checkpoint for epoch 6 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-6\n",
      "Saving checkpoint for epoch 7 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-7\n",
      "Saving checkpoint for epoch 8 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-8\n",
      "Saving checkpoint for epoch 9 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-9\n",
      "Saving checkpoint for epoch 10 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-10\n",
      "Saving checkpoint for epoch 11 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-11\n",
      "Saving checkpoint for epoch 12 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-12\n",
      "Saving checkpoint for epoch 13 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-13\n",
      "Saving checkpoint for epoch 14 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-14\n",
      "Saving checkpoint for epoch 15 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-15\n",
      "Saving checkpoint for epoch 16 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-16\n",
      "Saving checkpoint for epoch 17 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-17\n",
      "Saving checkpoint for epoch 18 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-18\n",
      "Saving checkpoint for epoch 19 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-19\n",
      "Saving checkpoint for epoch 20 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-20\n",
      "Saving checkpoint for epoch 21 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-21\n",
      "Saving checkpoint for epoch 22 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-22\n",
      "Saving checkpoint for epoch 23 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-23\n",
      "Saving checkpoint for epoch 24 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-24\n",
      "Saving checkpoint for epoch 25 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-25\n",
      "Saving checkpoint for epoch 26 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-26\n",
      "Saving checkpoint for epoch 27 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-27\n",
      "Saving checkpoint for epoch 28 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-28\n",
      "Saving checkpoint for epoch 29 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-29\n",
      "Saving checkpoint for epoch 30 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-30\n",
      "Saving checkpoint for epoch 31 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-31\n",
      "Saving checkpoint for epoch 32 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-32\n",
      "Saving checkpoint for epoch 33 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-33\n",
      "Saving checkpoint for epoch 34 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-34\n",
      "Saving checkpoint for epoch 35 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-35\n",
      "Saving checkpoint for epoch 36 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-36\n",
      "Saving checkpoint for epoch 37 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-37\n",
      "Saving checkpoint for epoch 38 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-38\n",
      "Saving checkpoint for epoch 39 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-39\n",
      "Saving checkpoint for epoch 40 at ./Trans/checkpoints/1layers_4d_2heads_8dff_20train_perc/1layers_4d_2heads_8dff/4layers_128d_8heads_512dff/ckpt-40\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "print(f\"already train {last_epoch} epochs。\")\n",
    "print(f\"last epochs：{min(0, last_epoch - EPOCHS)}\")\n",
    "\n",
    "for epoch in range(last_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "  \n",
    "    for (step_idx, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)  \n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [subword_encoder_en.vocab_size]\n",
    "    end_token = [subword_encoder_en.vocab_size + 1]\n",
    "  \n",
    "    inp_sentence = start_token + subword_encoder_en.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "\n",
    "    decoder_input = [subword_encoder_zh.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)  ## for batch dim\n",
    "  \n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "  \n",
    "    ## predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "        predictions = predictions[: , -1:, :]  \n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "\n",
    "        if tf.equal(predicted_id, subword_encoder_zh.vocab_size + 1):\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "        ## adding the last step output\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  \n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"China, India, and others have enjoyed continuing economic growth.\"\n",
    "\n",
    "predicted_seq, _ = evaluate(sentence)\n",
    "\n",
    "target_vocab_size = subword_encoder_zh.vocab_size\n",
    "predicted_seq_without_bos_eos = [idx for idx in predicted_seq if idx < target_vocab_size]\n",
    "predicted_sentence = subword_encoder_zh.decode(predicted_seq_without_bos_eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "印度和中国也是其他国家中国家中的经济增长。\n"
     ]
    }
   ],
   "source": [
    "print(predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
